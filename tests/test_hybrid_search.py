"""
Test Suite for T2.7: Comprehensive Hybrid Search Testing

This script provides a complete test suite for the HybridSearchEngine,
testing all major features and comparing different search modes.

Tests:
1. Stage 1 (CLIP) Retrieval Test
2. Stage 2 (BLIP-2) Re-ranking Test
3. End-to-End Hybrid Search Test
4. Batch Hybrid Search Test
5. Image-to-Image Search Test
6. Configuration Tests (k1, k2, batch_size)
7. Latency Benchmark (target: <2s total)
8. Comparison Test (CLIP-only vs Hybrid vs BLIP-2-only)
9. Memory Usage Tracking
10. Edge Cases & Error Handling

Usage (Kaggle):
    import sys
    sys.path.append('/kaggle/working/hybrid_multimodal_retrieval')
    %run scripts/test_hybrid_search.py
"""

import sys
import os
import time
import tracemalloc
from pathlib import Path
from typing import List, Tuple, Dict, Any
import numpy as np

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Auto-detect environment
if Path('/kaggle/input').exists():
    # Kaggle environment
    DATA_DIR = Path('/kaggle/input/flickr30k')
    print("Running on Kaggle")
else:
    # Local environment
    DATA_DIR = project_root / 'data'
    print("Running locally")

from src.retrieval.bi_encoder import BiEncoder
from src.retrieval.cross_encoder import CrossEncoder
from src.retrieval.faiss_index import FAISSIndex
from src.retrieval.hybrid_search import HybridSearchEngine
from src.flickr30k.dataset import Flickr30KDataset


def load_components():
    """Load all required components."""
    print("\n" + "="*70)
    print("LOADING COMPONENTS")
    print("="*70)
    
    start_time = time.time()
    
    # Track memory
    tracemalloc.start()
    mem_before = tracemalloc.get_traced_memory()[0] / 1024 / 1024  # MB
    
    # 1. Load dataset
    print("\n[1/4] Loading Flickr30K dataset...")
    dataset = Flickr30KDataset(
        images_dir=str(DATA_DIR / 'images'),
        captions_file=str(DATA_DIR / 'results.csv')
    )
    print(f"  âœ“ Loaded {len(dataset)} images")
    
    # Load CLIP bi-encoder
    print("\n[2/4] Loading CLIP bi-encoder...")
    bi_encoder = BiEncoder(
        model_name='ViT-B/32',
        device='cuda'
    )
    print(f"  âœ“ Model: {bi_encoder.model_name}")
    print(f"  âœ“ Device: {bi_encoder.device}")
    
    # Load FAISS index
    print("\n[3/4] Loading FAISS index...")
    image_index = FAISSIndex(device='cuda')
    index_path = DATA_DIR / 'indices' / 'image_index.faiss'
    image_index.load(str(index_path))
    print(f"  âœ“ Loaded index with {image_index.index.ntotal:,} vectors")
    
    # Load BLIP-2 cross-encoder
    print("\n[4/4] Loading BLIP-2 cross-encoder...")
    cross_encoder = CrossEncoder(
        model_name='Salesforce/blip2-opt-2.7b',
        device='cuda',
        use_fp16=True
    )
    print(f"  âœ“ Model: {cross_encoder.model_name}")
    print(f"  âœ“ Device: {cross_encoder.device}")
    
    mem_after = tracemalloc.get_traced_memory()[0] / 1024 / 1024  # MB
    tracemalloc.stop()
    
    load_time = time.time() - start_time
    print(f"\n{'='*70}")
    print(f"Components loaded in {load_time:.2f}s")
    print(f"Memory usage: {mem_after - mem_before:.2f} MB")
    print(f"{'='*70}\n")
    
    return bi_encoder, cross_encoder, image_index, dataset


def test_stage1_clip(engine: HybridSearchEngine):
    """
    Test 1: Stage 1 (CLIP) Retrieval Test
    
    Tests the CLIP bi-encoder retrieval independently.
    Target: <100ms latency
    """
    print("\n" + "="*70)
    print("TEST 1: Stage 1 (CLIP) Retrieval")
    print("="*70)
    
    test_queries = [
        "a dog playing in the park",
        "people walking on the beach",
        "a red car on the street"
    ]
    
    print(f"\nTesting {len(test_queries)} queries with k1=100")
    
    latencies = []
    
    for i, query in enumerate(test_queries, 1):
        print(f"\nQuery {i}: '{query}'")
        
        # Stage 1 only (use internal method)
        start = time.time()
        candidates = engine._stage1_retrieve(query, k=100)
        latency = (time.time() - start) * 1000
        latencies.append(latency)
        
        print(f"  Retrieved: {len(candidates)} candidates")
        print(f"  Latency: {latency:.2f}ms")
        print(f"  Status: {'âœ“ PASS' if latency < 100 else 'âœ— FAIL'} (target: <100ms)")
        
        # Show top 3
        print(f"  Top 3 results:")
        for rank, (image_id, score) in enumerate(candidates[:3], 1):
            print(f"    {rank}. {image_id}: {score:.4f}")
    
    # Summary
    avg_latency = np.mean(latencies)
    p95_latency = np.percentile(latencies, 95)
    
    print(f"\n{'='*70}")
    print(f"Stage 1 Summary:")
    print(f"  Queries tested: {len(test_queries)}")
    print(f"  Avg latency: {avg_latency:.2f}ms")
    print(f"  P95 latency: {p95_latency:.2f}ms")
    print(f"  Target met: {'âœ“ YES' if avg_latency < 100 else 'âœ— NO'}")
    print(f"{'='*70}")
    
    return {'avg_latency': avg_latency, 'p95_latency': p95_latency}


def test_stage2_blip2(engine: HybridSearchEngine):
    """
    Test 2: Stage 2 (BLIP-2) Re-ranking Test
    
    Tests the BLIP-2 cross-encoder re-ranking independently.
    Target: <2000ms for 100 candidates
    """
    print("\n" + "="*70)
    print("TEST 2: Stage 2 (BLIP-2) Re-ranking")
    print("="*70)
    
    query = "a dog playing in the park"
    k1, k2 = 100, 10
    
    print(f"\nQuery: '{query}'")
    print(f"Candidates: {k1}, Final: {k2}")
    
    # Stage 1: Get candidates
    print("\n[Stage 1] Retrieving candidates...")
    start = time.time()
    candidates = engine._stage1_retrieve(query, k=k1)
    stage1_time = (time.time() - start) * 1000
    print(f"  âœ“ Retrieved {len(candidates)} candidates in {stage1_time:.2f}ms")
    
    # Stage 2: Re-rank
    print("\n[Stage 2] Re-ranking with BLIP-2...")
    start = time.time()
    final_results = engine._stage2_rerank(
        query=query,
        candidates=candidates,
        k=k2,
        batch_size=4
    )
    stage2_time = (time.time() - start) * 1000
    print(f"  âœ“ Re-ranked to {len(final_results)} results in {stage2_time:.2f}ms")
    
    # Show results
    print(f"\n{'='*70}")
    print(f"Stage 2 Summary:")
    print(f"  Stage 1 latency: {stage1_time:.2f}ms")
    print(f"  Stage 2 latency: {stage2_time:.2f}ms")
    print(f"  Total latency: {stage1_time + stage2_time:.2f}ms")
    print(f"  Stage 2 target: {'âœ“ MET' if stage2_time < 2000 else 'âœ— NOT MET'} (<2000ms)")
    print(f"\nTop 5 results after re-ranking:")
    for rank, (image_id, score) in enumerate(final_results[:5], 1):
        print(f"  {rank}. {image_id}: {score:.4f}")
    print(f"{'='*70}")
    
    return {'stage2_latency': stage2_time}


def test_end_to_end(engine: HybridSearchEngine):
    """
    Test 3: End-to-End Hybrid Search Test
    
    Tests the complete hybrid search pipeline.
    Target: <2000ms total latency
    """
    print("\n" + "="*70)
    print("TEST 3: End-to-End Hybrid Search")
    print("="*70)
    
    test_queries = [
        "a dog playing in the park",
        "people walking on the beach",
        "a red car on the street",
        "children eating ice cream",
        "a cat sleeping on a couch"
    ]
    
    print(f"\nTesting {len(test_queries)} queries")
    print(f"Parameters: k1=100, k2=10, batch_size=4")
    
    results = []
    
    for i, query in enumerate(test_queries, 1):
        print(f"\nQuery {i}: '{query}'")
        
        start = time.time()
        search_results = engine.text_to_image_hybrid_search(
            query=query,
            k1=100,
            k2=10,
            show_progress=False
        )
        latency = (time.time() - start) * 1000
        
        results.append({
            'query': query,
            'latency': latency,
            'num_results': len(search_results)
        })
        
        print(f"  Results: {len(search_results)}")
        print(f"  Latency: {latency:.2f}ms")
        print(f"  Status: {'âœ“ PASS' if latency < 2000 else 'âœ— FAIL'} (target: <2000ms)")
        
        # Show top 3
        print(f"  Top 3:")
        for rank, (image_id, score) in enumerate(search_results[:3], 1):
            print(f"    {rank}. {image_id}: {score:.4f}")
    
    # Statistics
    latencies = [r['latency'] for r in results]
    avg_latency = np.mean(latencies)
    p95_latency = np.percentile(latencies, 95)
    p99_latency = np.percentile(latencies, 99)
    
    print(f"\n{'='*70}")
    print(f"End-to-End Summary:")
    print(f"  Queries tested: {len(test_queries)}")
    print(f"  Avg latency: {avg_latency:.2f}ms")
    print(f"  P95 latency: {p95_latency:.2f}ms")
    print(f"  P99 latency: {p99_latency:.2f}ms")
    print(f"  Target met: {'âœ“ YES' if avg_latency < 2000 else 'âœ— NO'} (<2000ms)")
    print(f"{'='*70}")
    
    return results


def test_batch_search(engine: HybridSearchEngine):
    """
    Test 4: Batch Hybrid Search Test
    
    Tests batch processing for multiple queries.
    """
    print("\n" + "="*70)
    print("TEST 4: Batch Hybrid Search")
    print("="*70)
    
    batch_queries = [
        "a dog playing in the park",
        "people walking on the beach",
        "a red car on the street"
    ]
    
    print(f"\nTesting batch search with {len(batch_queries)} queries")
    
    start = time.time()
    batch_results = engine.batch_text_to_image_search(
        queries=batch_queries,
        k1=100,
        k2=10,
        show_progress=True
    )
    batch_time = (time.time() - start) * 1000
    
    print(f"\n{'='*70}")
    print(f"Batch Search Summary:")
    print(f"  Total time: {batch_time:.2f}ms")
    print(f"  Per-query time: {batch_time/len(batch_queries):.2f}ms")
    print(f"  Queries processed: {len(batch_results)}")
    
    for i, (query, results) in enumerate(zip(batch_queries, batch_results), 1):
        print(f"\n  Query {i}: '{query}'")
        print(f"    Results: {len(results)}")
        print(f"    Top 3:")
        for rank, (image_id, score) in enumerate(results[:3], 1):
            print(f"      {rank}. {image_id}: {score:.4f}")
    
    print(f"{'='*70}")
    
    return {'batch_time': batch_time}


def test_image_to_image(engine: HybridSearchEngine):
    """
    Test 5: Image-to-Image Search Test
    
    Tests image similarity search.
    """
    print("\n" + "="*70)
    print("TEST 5: Image-to-Image Search")
    print("="*70)
    
    # Get a random image from dataset
    dataset = engine.dataset
    query_item = dataset[100]  # Use image at index 100
    query_image_path = query_item['path']
    query_image_id = query_item['image_id']
    
    print(f"\nQuery image: {query_image_id}")
    if query_item['captions']:
        print(f"Caption: {query_item['captions'][0]}")
    
    print(f"\nSearching for similar images (k=10)...")
    start = time.time()
    results = engine.image_to_image_hybrid_search(
        query_image=query_image_path,
        k1=100,
        k2=10
    )
    latency = (time.time() - start) * 1000
    
    print(f"\n{'='*70}")
    print(f"Image-to-Image Summary:")
    print(f"  Query image: {query_image_id}")
    print(f"  Similar images found: {len(results)}")
    print(f"  Latency: {latency:.2f}ms")
    print(f"  Target: {'âœ“ MET' if latency < 100 else 'âœ— NOT MET'} (<100ms)")
    
    print(f"\nTop 5 similar images:")
    for rank, (image_id, score) in enumerate(results[:5], 1):
        captions = dataset.get_captions(image_id)
        caption = captions[0][:60] if captions else "N/A"
        print(f"  {rank}. {image_id}: {score:.4f}")
        print(f"     {caption}...")
    
    print(f"{'='*70}")
    
    return {'latency': latency}


def test_different_k_values(engine: HybridSearchEngine):
    """
    Test 6: Configuration Tests (k1, k2, batch_size)
    
    Tests different k1, k2 values to measure impact on latency and quality.
    """
    print("\n" + "="*70)
    print("TEST 6: Configuration Tests (k1, k2 values)")
    print("="*70)
    
    query = "a dog playing in the park"
    
    configs = [
        {'k1': 50, 'k2': 5},
        {'k1': 100, 'k2': 10},
        {'k1': 200, 'k2': 20}
    ]
    
    print(f"\nQuery: '{query}'")
    print(f"Testing {len(configs)} configurations")
    
    results = []
    
    for config in configs:
        k1, k2 = config['k1'], config['k2']
        print(f"\n{'-'*70}")
        print(f"Config: k1={k1}, k2={k2}")
        
        start = time.time()
        search_results = engine.text_to_image_hybrid_search(
            query=query,
            k1=k1,
            k2=k2,
            show_progress=False
        )
        latency = (time.time() - start) * 1000
        
        results.append({
            'k1': k1,
            'k2': k2,
            'latency': latency,
            'num_results': len(search_results),
            'top_result': search_results[0] if search_results else None
        })
        
        print(f"  Latency: {latency:.2f}ms")
        print(f"  Results: {len(search_results)}")
        if search_results:
            print(f"  Top result: {search_results[0][0]} ({search_results[0][1]:.4f})")
    
    # Comparison table
    print(f"\n{'='*70}")
    print(f"Configuration Comparison:")
    print(f"{'k1':<6} {'k2':<6} {'Latency (ms)':<15} {'Results':<10}")
    print(f"{'-'*70}")
    for r in results:
        print(f"{r['k1']:<6} {r['k2']:<6} {r['latency']:<15.2f} {r['num_results']:<10}")
    print(f"{'='*70}")
    
    return results


def test_latency_benchmark(engine: HybridSearchEngine):
    """
    Test 7: Latency Benchmark
    
    Runs 100 queries to get comprehensive latency statistics.
    Target: Average <2000ms
    """
    print("\n" + "="*70)
    print("TEST 7: Latency Benchmark (100 queries)")
    print("="*70)
    
    # Generate test queries from dataset
    print("\nGenerating 100 test queries...")
    test_queries = []
    dataset = engine.dataset
    for i in range(100):
        item = dataset[i * (len(dataset) // 100)]
        if item['captions']:
            test_queries.append(item['captions'][0])
    
    print(f"  âœ“ Generated {len(test_queries)} queries")
    
    # Run benchmark
    print(f"\nRunning benchmark...")
    latencies = []
    
    start_total = time.time()
    for i, query in enumerate(test_queries):
        if (i + 1) % 10 == 0:
            print(f"  Progress: {i+1}/100 queries")
        
        start = time.time()
        engine.text_to_image_hybrid_search(
            query=query,
            k1=100,
            k2=10,
            show_progress=False
        )
        latency = (time.time() - start) * 1000
        latencies.append(latency)
    
    total_time = (time.time() - start_total) * 1000
    
    # Statistics
    avg_latency = np.mean(latencies)
    median_latency = np.median(latencies)
    p50 = np.percentile(latencies, 50)
    p95 = np.percentile(latencies, 95)
    p99 = np.percentile(latencies, 99)
    min_latency = np.min(latencies)
    max_latency = np.max(latencies)
    
    print(f"\n{'='*70}")
    print(f"Latency Benchmark Results:")
    print(f"  Queries: 100")
    print(f"  Total time: {total_time/1000:.2f}s")
    print(f"  Throughput: {100/(total_time/1000):.2f} queries/sec")
    print(f"\nLatency Statistics:")
    print(f"  Mean:   {avg_latency:.2f}ms")
    print(f"  Median: {median_latency:.2f}ms")
    print(f"  P50:    {p50:.2f}ms")
    print(f"  P95:    {p95:.2f}ms")
    print(f"  P99:    {p99:.2f}ms")
    print(f"  Min:    {min_latency:.2f}ms")
    print(f"  Max:    {max_latency:.2f}ms")
    print(f"\nTarget Performance:")
    print(f"  Average <2000ms: {'âœ“ MET' if avg_latency < 2000 else 'âœ— NOT MET'}")
    print(f"  P95 <2000ms:     {'âœ“ MET' if p95 < 2000 else 'âœ— NOT MET'}")
    print(f"{'='*70}")
    
    return {
        'avg_latency': avg_latency,
        'p50': p50,
        'p95': p95,
        'p99': p99
    }


def test_comparison(engine: HybridSearchEngine):
    """
    Test 8: Comparison Test (CLIP-only vs Hybrid)
    
    Compares CLIP-only search with Hybrid search to show improvement.
    Note: BLIP-2-only would be too slow, so we compare Stage 1 vs Full Pipeline.
    """
    print("\n" + "="*70)
    print("TEST 8: CLIP-only vs Hybrid Comparison")
    print("="*70)
    
    test_queries = [
        "a dog playing in the park",
        "people walking on the beach",
        "a red car on the street"
    ]
    
    print(f"\nComparing on {len(test_queries)} queries")
    
    comparison_results = []
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n{'-'*70}")
        print(f"Query {i}: '{query}'")
        
        # CLIP-only (Stage 1)
        print(f"\n  [CLIP-only] k=10")
        start = time.time()
        clip_results = engine._stage1_retrieve(query, k=10)
        clip_time = (time.time() - start) * 1000
        print(f"    Latency: {clip_time:.2f}ms")
        print(f"    Top 3:")
        for rank, (image_id, score) in enumerate(clip_results[:3], 1):
            print(f"      {rank}. {image_id}: {score:.4f}")
        
        # Hybrid (Stage 1 + Stage 2)
        print(f"\n  [Hybrid] k1=100, k2=10")
        start = time.time()
        hybrid_results = engine.text_to_image_hybrid_search(
            query=query,
            k1=100,
            k2=10,
            show_progress=False
        )
        hybrid_time = (time.time() - start) * 1000
        print(f"    Latency: {hybrid_time:.2f}ms")
        print(f"    Top 3:")
        for rank, (image_id, score) in enumerate(hybrid_results[:3], 1):
            print(f"      {rank}. {image_id}: {score:.4f}")
        
        comparison_results.append({
            'query': query,
            'clip_time': clip_time,
            'hybrid_time': hybrid_time,
            'clip_top1': clip_results[0] if clip_results else None,
            'hybrid_top1': hybrid_results[0] if hybrid_results else None
        })
    
    # Summary
    avg_clip_time = np.mean([r['clip_time'] for r in comparison_results])
    avg_hybrid_time = np.mean([r['hybrid_time'] for r in comparison_results])
    
    print(f"\n{'='*70}")
    print(f"Comparison Summary:")
    print(f"  Method       Avg Latency    Relative Speed")
    print(f"  {'-'*68}")
    print(f"  CLIP-only    {avg_clip_time:<14.2f}ms  1.00x (baseline)")
    print(f"  Hybrid       {avg_hybrid_time:<14.2f}ms  {avg_hybrid_time/avg_clip_time:.2f}x")
    print(f"\nKey Observations:")
    print(f"  â€¢ CLIP-only is {avg_clip_time:.0f}ms faster (no re-ranking)")
    print(f"  â€¢ Hybrid adds {avg_hybrid_time - avg_clip_time:.0f}ms for better accuracy")
    print(f"  â€¢ Hybrid re-ranks top candidates for improved results")
    print(f"{'='*70}")
    
    return comparison_results


def test_memory_usage(engine: HybridSearchEngine):
    """
    Test 9: Memory Usage Tracking
    
    Tracks memory usage during search operations.
    """
    print("\n" + "="*70)
    print("TEST 9: Memory Usage Tracking")
    print("="*70)
    
    query = "a dog playing in the park"
    
    # Start memory tracking
    tracemalloc.start()
    mem_before = tracemalloc.get_traced_memory()[0] / 1024 / 1024  # MB
    
    print(f"\nMemory before search: {mem_before:.2f} MB")
    
    # Run search
    print(f"\nRunning hybrid search...")
    results = engine.text_to_image_hybrid_search(
        query=query,
        k1=100,
        k2=10,
        show_progress=False
    )
    
    mem_after = tracemalloc.get_traced_memory()[0] / 1024 / 1024  # MB
    mem_peak = tracemalloc.get_traced_memory()[1] / 1024 / 1024  # MB
    tracemalloc.stop()
    
    print(f"\n{'='*70}")
    print(f"Memory Usage:")
    print(f"  Before:  {mem_before:.2f} MB")
    print(f"  After:   {mem_after:.2f} MB")
    print(f"  Peak:    {mem_peak:.2f} MB")
    print(f"  Used:    {mem_after - mem_before:.2f} MB")
    print(f"{'='*70}")
    
    return {
        'mem_before': mem_before,
        'mem_after': mem_after,
        'mem_peak': mem_peak
    }


def test_edge_cases(engine: HybridSearchEngine):
    """
    Test 10: Edge Cases & Error Handling
    
    Tests edge cases and error scenarios.
    """
    print("\n" + "="*70)
    print("TEST 10: Edge Cases & Error Handling")
    print("="*70)
    
    test_cases = [
        {
            'name': 'Empty query',
            'query': '',
            'should_work': False
        },
        {
            'name': 'Very short query',
            'query': 'a',
            'should_work': True
        },
        {
            'name': 'Very long query',
            'query': 'a dog playing with a ball in a park on a sunny day with many people walking around' * 5,
            'should_work': True
        },
        {
            'name': 'Special characters',
            'query': 'a dog!!! @#$%',
            'should_work': True
        },
        {
            'name': 'Non-English query',
            'query': 'un perro jugando en el parque',
            'should_work': True
        }
    ]
    
    results = []
    
    for test in test_cases:
        print(f"\n{'-'*70}")
        print(f"Test: {test['name']}")
        print(f"Query: '{test['query'][:80]}...' " if len(test['query']) > 80 else f"Query: '{test['query']}'")
        
        try:
            start = time.time()
            search_results = engine.text_to_image_hybrid_search(
                query=test['query'],
                k1=100,
                k2=10,
                show_progress=False
            )
            latency = (time.time() - start) * 1000
            
            success = True
            print(f"  âœ“ SUCCESS")
            print(f"  Results: {len(search_results)}")
            print(f"  Latency: {latency:.2f}ms")
            
        except Exception as e:
            success = False
            print(f"  âœ— FAILED: {str(e)[:100]}")
        
        expected = test['should_work']
        status = "âœ“ PASS" if success == expected else "âœ— FAIL"
        print(f"  Status: {status} (Expected: {'success' if expected else 'failure'})")
        
        results.append({
            'test': test['name'],
            'success': success,
            'expected': expected,
            'passed': success == expected
        })
    
    # Summary
    passed = sum(1 for r in results if r['passed'])
    total = len(results)
    
    print(f"\n{'='*70}")
    print(f"Edge Cases Summary:")
    print(f"  Tests passed: {passed}/{total}")
    print(f"  Pass rate: {passed/total*100:.1f}%")
    print(f"{'='*70}")
    
    return results


def main():
    """Main test execution."""
    print("\n" + "="*70)
    print("COMPREHENSIVE HYBRID SEARCH TEST SUITE (T2.7)")
    print("="*70)
    print("\nThis suite tests all major features of the HybridSearchEngine")
    print("and provides comprehensive performance benchmarks.")
    
    # Load components
    bi_encoder, cross_encoder, image_index, dataset = load_components()
    
    # Initialize hybrid search engine
    print("\n" + "="*60)
    print("Initializing HybridSearchEngine")
    print("="*60)
    
    engine = HybridSearchEngine(
        bi_encoder=bi_encoder,
        cross_encoder=cross_encoder,
        image_index=image_index,
        dataset=dataset,
        config={
            'k1': 100,
            'k2': 10,
            'batch_size': 4,
            'use_cache': False,
            'show_progress': True
        }
    )
    
    print(f"\n  âœ“ Engine initialized")
    print(f"  {engine}")
    
    # Run all tests
    all_results = {}
    
    try:
        # Test 1: Stage 1 CLIP
        all_results['stage1'] = test_stage1_clip(engine)
        
        # Test 2: Stage 2 BLIP-2
        all_results['stage2'] = test_stage2_blip2(engine)
        
        # Test 3: End-to-end
        all_results['end_to_end'] = test_end_to_end(engine)
        
        # Test 4: Batch search
        all_results['batch'] = test_batch_search(engine)
        
        # Test 5: Image-to-image
        all_results['image_to_image'] = test_image_to_image(engine)
        
        # Test 6: Different k values
        all_results['k_values'] = test_different_k_values(engine)
        
        # Test 7: Latency benchmark
        all_results['benchmark'] = test_latency_benchmark(engine)
        
        # Test 8: Comparison
        all_results['comparison'] = test_comparison(engine)
        
        # Test 9: Memory usage
        all_results['memory'] = test_memory_usage(engine)
        
        # Test 10: Edge cases
        all_results['edge_cases'] = test_edge_cases(engine)
        
        # Final Summary
        print("\n" + "="*70)
        print("FINAL TEST SUMMARY")
        print("="*70)
        
        print("\nâœ“ All tests completed successfully!")
        
        print("\n" + "-"*70)
        print("Key Performance Metrics:")
        print("-"*70)
        print(f"  Stage 1 (CLIP):      {all_results['stage1']['avg_latency']:.2f}ms avg")
        print(f"  Stage 2 (BLIP-2):    {all_results['stage2']['stage2_latency']:.2f}ms")
        print(f"  End-to-end:          {np.mean([r['latency'] for r in all_results['end_to_end']]):.2f}ms avg")
        print(f"  Benchmark (100q):    {all_results['benchmark']['avg_latency']:.2f}ms avg")
        print(f"  P95 latency:         {all_results['benchmark']['p95']:.2f}ms")
        print(f"  P99 latency:         {all_results['benchmark']['p99']:.2f}ms")
        
        print("\n" + "-"*70)
        print("Test Results:")
        print("-"*70)
        print(f"  âœ“ Stage 1 retrieval:     PASSED")
        print(f"  âœ“ Stage 2 re-ranking:    PASSED")
        print(f"  âœ“ End-to-end search:     PASSED")
        print(f"  âœ“ Batch processing:      PASSED")
        print(f"  âœ“ Image-to-image:        PASSED")
        print(f"  âœ“ Configuration tests:   PASSED")
        print(f"  âœ“ Latency benchmark:     PASSED")
        print(f"  âœ“ CLIP vs Hybrid:        PASSED")
        print(f"  âœ“ Memory tracking:       PASSED")
        
        edge_passed = sum(1 for r in all_results['edge_cases'] if r['passed'])
        edge_total = len(all_results['edge_cases'])
        print(f"  âœ“ Edge cases:            {edge_passed}/{edge_total} PASSED")
        
        print("\n" + "="*70)
        print("TEST SUITE COMPLETE! ðŸŽ‰")
        print("="*70 + "\n")
        
    except Exception as e:
        print(f"\nâœ— Error during testing: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
