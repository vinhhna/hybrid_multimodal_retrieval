{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f03a0f7",
   "metadata": {},
   "source": [
    "# Phase 3: BLIP-2 Cross-Encoder Exploration\n",
    "\n",
    "**Notebook**: BLIP-2 Integration and Testing  \n",
    "**Phase**: 3 - Cross-Encoder Reranking  \n",
    "**Week**: 1 - BLIP-2 Integration  \n",
    "**Created**: October 28, 2025\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Objectives\n",
    "\n",
    "This notebook explores and validates the BLIP-2 cross-encoder integration for Phase 3:\n",
    "\n",
    "1. **Setup & Installation**: Verify BLIP-2 installation and dependencies\n",
    "2. **Model Loading**: Load BLIP-2 model and test GPU functionality\n",
    "3. **Single Pair Scoring**: Test scoring on individual query-image pairs\n",
    "4. **CLIP vs BLIP-2 Comparison**: Compare bi-encoder vs cross-encoder scores\n",
    "5. **Batch Processing**: Test and optimize batch scoring\n",
    "6. **Memory Optimization**: Handle GPU memory constraints\n",
    "7. **Quality Validation**: Validate scoring quality with diverse queries\n",
    "8. **Performance Benchmarks**: Measure speed and throughput\n",
    "9. **Findings Summary**: Document insights and next steps\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Expected Outcomes\n",
    "\n",
    "- ‚úÖ BLIP-2 loads successfully on GPU\n",
    "- ‚úÖ Can score query-candidate pairs\n",
    "- ‚úÖ Batch processing works efficiently\n",
    "- ‚úÖ Memory constraints handled gracefully\n",
    "- ‚úÖ BLIP-2 provides better quality than CLIP alone\n",
    "- ‚úÖ Performance metrics documented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359cacbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "# Check PyTorch and CUDA\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"‚úì Core libraries imported successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aa60f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CrossEncoder\n",
    "from retrieval.cross_encoder import CrossEncoder\n",
    "\n",
    "print(\"Loading BLIP-2 Cross-Encoder...\")\n",
    "print(\"This may take a few minutes on first run (downloading model weights)...\")\n",
    "\n",
    "encoder = CrossEncoder(\n",
    "    model_name='blip2_opt',\n",
    "    model_type='pretrain_opt2.7b',\n",
    "    use_fp16=True\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì CrossEncoder initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2f21f0",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Summary & Next Steps\n",
    "\n",
    "### ‚úÖ Week 1 Accomplishments\n",
    "\n",
    "Document what was achieved in Week 1:\n",
    "\n",
    "- **Model Loading**: BLIP-2 successfully loaded\n",
    "- **Single Pair Scoring**: Functional and validated\n",
    "- **Batch Processing**: Optimized batch sizes identified\n",
    "- **Memory Management**: GPU constraints handled\n",
    "- **Quality**: BLIP-2 provides different (potentially better) scores than CLIP\n",
    "- **Performance**: Benchmarked on 100 pairs\n",
    "\n",
    "### üìã Week 2 Tasks\n",
    "\n",
    "Next week (Nov 4-30):\n",
    "1. Implement re-ranking function\n",
    "2. Create `HybridRetriever` class\n",
    "3. Integrate bi-encoder + cross-encoder pipeline\n",
    "4. Implement evaluation metrics (Recall@K, Precision@K, MRR)\n",
    "5. Build demo notebook showing hybrid retrieval\n",
    "6. Compare hybrid vs bi-encoder only\n",
    "\n",
    "### üéØ Success Criteria\n",
    "\n",
    "- [ ] Re-ranking time < 2 seconds for top-100\n",
    "- [ ] Recall@10 improvement: +15-20% over bi-encoder\n",
    "- [ ] Precision@10 > 60%\n",
    "- [ ] Full hybrid pipeline working end-to-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fe5954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: Score 100 pairs (simulating reranking top-100)\n",
    "n_benchmark = 100\n",
    "benchmark_images = list(Path('../data/images').glob('*.jpg'))[:n_benchmark]\n",
    "benchmark_queries = [\"A photograph\"] * n_benchmark  # Same query for all\n",
    "\n",
    "print(f\"Benchmarking with {n_benchmark} pairs...\")\n",
    "print(\"This simulates reranking top-100 bi-encoder results\\n\")\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "scores = encoder.score_pairs(\n",
    "    benchmark_queries,\n",
    "    benchmark_images,\n",
    "    batch_size=8,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"\\n‚úì Benchmark Results:\")\n",
    "print(f\"  Total time: {elapsed:.2f}s\")\n",
    "print(f\"  Time per pair: {elapsed/n_benchmark*1000:.1f}ms\")\n",
    "print(f\"  Throughput: {n_benchmark/elapsed:.1f} pairs/second\")\n",
    "print(f\"\\nTarget for Phase 3: < 2 seconds for 100 pairs\")\n",
    "print(f\"Status: {'‚úì PASS' if elapsed < 2 else '‚ö† NEEDS OPTIMIZATION'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d781459c",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Performance Benchmarks\n",
    "\n",
    "Measure performance metrics for Week 1 deliverables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81b79fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare on same pairs\n",
    "comparison_queries = test_queries[:5]\n",
    "comparison_images = test_images[:5]\n",
    "\n",
    "print(\"Getting CLIP scores...\")\n",
    "clip_img_embs = clip_encoder.encode_images(comparison_images, show_progress=False)\n",
    "clip_text_embs = clip_encoder.encode_texts(comparison_queries, show_progress=False)\n",
    "clip_scores = (clip_img_embs * clip_text_embs).sum(axis=1)\n",
    "\n",
    "print(\"Getting BLIP-2 scores...\")\n",
    "blip2_scores = encoder.score_pairs(\n",
    "    comparison_queries,\n",
    "    comparison_images,\n",
    "    batch_size=4,\n",
    "    show_progress=False\n",
    ")\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Query': comparison_queries,\n",
    "    'Image': [img.name for img in comparison_images],\n",
    "    'CLIP Score': clip_scores,\n",
    "    'BLIP-2 Score': blip2_scores,\n",
    "    'Difference': blip2_scores - clip_scores\n",
    "})\n",
    "\n",
    "print(\"\\nCLIP vs BLIP-2 Comparison:\")\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87273dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP bi-encoder for comparison\n",
    "from retrieval.bi_encoder import BiEncoder\n",
    "\n",
    "clip_encoder = BiEncoder(model_name='ViT-B-32', pretrained='openai')\n",
    "print(\"‚úì CLIP bi-encoder loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b041677d",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ CLIP vs BLIP-2 Comparison\n",
    "\n",
    "Compare bi-encoder (CLIP) scores with cross-encoder (BLIP-2) scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d234856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different batch sizes\n",
    "batch_sizes = [2, 4, 8]\n",
    "\n",
    "results = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"\\nTesting batch size: {batch_size}\")\n",
    "    \n",
    "    import time\n",
    "    start = time.time()\n",
    "    \n",
    "    scores = encoder.score_pairs(\n",
    "        test_queries,\n",
    "        test_images,\n",
    "        batch_size=batch_size,\n",
    "        show_progress=True\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    results.append({\n",
    "        'batch_size': batch_size,\n",
    "        'total_time': elapsed,\n",
    "        'time_per_pair': elapsed / n_pairs,\n",
    "        'mean_score': scores.mean(),\n",
    "        'std_score': scores.std()\n",
    "    })\n",
    "    \n",
    "    print(f\"  Time: {elapsed:.2f}s ({elapsed/n_pairs*1000:.1f}ms per pair)\")\n",
    "    print(f\"  Scores: mean={scores.mean():.4f}, std={scores.std():.4f}\")\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nBatch Size Comparison:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd5c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare batch test data\n",
    "n_pairs = 10\n",
    "test_images = list(Path('../data/images').glob('*.jpg'))[:n_pairs]\n",
    "test_queries = [\n",
    "    \"A photograph of people\",\n",
    "    \"An outdoor scene\",\n",
    "    \"Children playing\",\n",
    "    \"A colorful image\",\n",
    "    \"An action scene\",\n",
    "    \"A landscape view\",\n",
    "    \"People in a setting\",\n",
    "    \"An indoor environment\",\n",
    "    \"A busy scene\",\n",
    "    \"A peaceful moment\"\n",
    "][:n_pairs]\n",
    "\n",
    "print(f\"Testing batch scoring with {n_pairs} pairs...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c07b4d",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Batch Scoring Tests\n",
    "\n",
    "Test batch processing with different batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173470c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different queries\n",
    "test_queries = [\n",
    "    \"A dog playing in the park\",\n",
    "    \"People at a beach\",\n",
    "    \"A colorful outdoor scene\",\n",
    "    \"Random unrelated text xyz123\",\n",
    "]\n",
    "\n",
    "print(\"Scoring single pairs...\")\n",
    "print(f\"Image: {test_image.name}\\n\")\n",
    "\n",
    "for query in test_queries:\n",
    "    score = encoder.score_pair(query, test_image)\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"Score: {score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcf587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a test image\n",
    "test_images = list(Path('../data/images').glob('*.jpg'))[:5]\n",
    "test_image = test_images[0]\n",
    "\n",
    "# Load and display image\n",
    "img = Image.open(test_image)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title(f\"Test Image: {test_image.name}\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image: {test_image.name}\")\n",
    "print(f\"Size: {img.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493ef8e9",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Single Pair Scoring\n",
    "\n",
    "Test scoring on a single query-image pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6969bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model information\n",
    "model_info = encoder.get_model_info()\n",
    "\n",
    "print(\"Model Information:\")\n",
    "print(f\"  Device: {model_info['device']}\")\n",
    "print(f\"  FP16: {model_info['use_fp16']}\")\n",
    "print(f\"  Batch size: {model_info['batch_size']}\")\n",
    "print(f\"  Max batch size: {model_info['max_batch_size']}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU Memory:\")\n",
    "    print(f\"  Allocated: {model_info['memory_allocated'] / 1e9:.2f} GB\")\n",
    "    print(f\"  Reserved: {model_info['memory_reserved'] / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21438e1e",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Model Loading & GPU Testing\n",
    "\n",
    "Load the BLIP-2 cross-encoder model and verify it's on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d2a8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset for testing\n",
    "from flickr30k import Flickr30KDataset\n",
    "\n",
    "dataset = Flickr30KDataset(\n",
    "    images_dir='../data/images',\n",
    "    captions_file='../data/results.csv'\n",
    ")\n",
    "\n",
    "print(f\"‚úì Dataset loaded: {len(dataset)} captions, {dataset.num_images} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42619fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check BLIP-2 installation\n",
    "try:\n",
    "    import lavis\n",
    "    from lavis.models import load_model_and_preprocess\n",
    "    print(\"‚úì salesforce-lavis installed successfully\")\n",
    "    print(f\"LAVIS version: {lavis.__version__ if hasattr(lavis, '__version__') else 'unknown'}\")\n",
    "except ImportError as e:\n",
    "    print(\"‚úó salesforce-lavis not installed\")\n",
    "    print(\"Install with: pip install salesforce-lavis\")\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfa8ab2",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup & Installation\n",
    "\n",
    "Check that all dependencies are installed and CUDA is available."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
