{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e41b5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KAGGLE SETUP - Cell 2: Setup data paths\n",
    "from pathlib import Path\n",
    "\n",
    "# Set paths based on Kaggle dataset location\n",
    "IMAGES_DIR = Path('/kaggle/input/flickr30k/data/images')\n",
    "CAPTIONS_FILE = Path('/kaggle/input/flickr30k/data/results.csv')\n",
    "\n",
    "# Verify paths\n",
    "print(f\"Images dir exists: {IMAGES_DIR.exists()} - {IMAGES_DIR}\")\n",
    "print(f\"Captions file exists: {CAPTIONS_FILE.exists()} - {CAPTIONS_FILE}\")\n",
    "\n",
    "if IMAGES_DIR.exists():\n",
    "    num_images = len(list(IMAGES_DIR.glob('*.jpg')))\n",
    "    print(f\"Found {num_images} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f7af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KAGGLE SETUP - Cell 1: Clone repository and install dependencies\n",
    "!rm -rf hybrid_multimodal_retrieval\n",
    "!git clone https://github.com/vinhhna/hybrid_multimodal_retrieval.git\n",
    "%cd hybrid_multimodal_retrieval\n",
    "!pip install -q transformers accelerate open-clip-torch pyyaml tqdm pillow faiss-cpu\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07933a6f",
   "metadata": {},
   "source": [
    "## üöÄ Kaggle Setup - RUN THESE FIRST!\n",
    "\n",
    "**Important:** Execute the two cells above before proceeding with the rest of the notebook.\n",
    "\n",
    "These cells will:\n",
    "1. Clone the repository and install all dependencies\n",
    "2. Set up the correct data paths for Kaggle environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6821d4f5",
   "metadata": {},
   "source": [
    "# Phase 3: BLIP-2 Cross-Encoder Exploration\n",
    "\n",
    "**Notebook**: BLIP-2 Integration and Testing  \n",
    "**Phase**: 3 - Cross-Encoder Reranking  \n",
    "**Week**: 1 - BLIP-2 Integration  \n",
    "**Created**: October 28, 2025\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Objectives\n",
    "\n",
    "This notebook explores and validates the BLIP-2 cross-encoder integration:\n",
    "\n",
    "1. **Setup**: Verify dependencies and GPU availability\n",
    "2. **Model Loading**: Load BLIP-2 from Hugging Face\n",
    "3. **Single Pair Scoring**: Test on individual query-image pairs\n",
    "4. **Batch Processing**: Optimize batch sizes for performance\n",
    "5. **CLIP Comparison**: Compare with bi-encoder scores\n",
    "6. **Performance**: Benchmark speed and throughput\n",
    "7. **Quality**: Validate scoring with diverse queries\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Success Criteria\n",
    "\n",
    "- ‚úÖ BLIP-2 loads successfully on GPU\n",
    "- ‚úÖ Scoring produces interpretable results (0-1 range)\n",
    "- ‚úÖ Batch processing < 2 seconds for 100 pairs\n",
    "- ‚úÖ BLIP-2 provides different perspective than CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dba441",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7c3b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core dependencies\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Checking dependencies...\\n\")\n",
    "\n",
    "# PyTorch\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"‚úì PyTorch: {torch.__version__}\")\n",
    "    print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "except ImportError:\n",
    "    print(\"‚úó PyTorch not installed\")\n",
    "\n",
    "# Transformers (for BLIP-2)\n",
    "try:\n",
    "    from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "    import transformers\n",
    "    print(f\"\\n‚úì Transformers: {transformers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"\\n‚úó Transformers not installed\")\n",
    "    print(\"Install with: pip install transformers accelerate\")\n",
    "\n",
    "# Other dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"\\n‚úì All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a07e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify paths are set from Kaggle setup cells above\n",
    "print(f\"Images dir: {IMAGES_DIR}\")\n",
    "print(f\"Captions file: {CAPTIONS_FILE}\")\n",
    "print(f\"Images exist: {IMAGES_DIR.exists()}\")\n",
    "print(f\"Captions exist: {CAPTIONS_FILE.exists()}\")\n",
    "\n",
    "if IMAGES_DIR.exists():\n",
    "    num_images = len(list(IMAGES_DIR.glob('*.jpg')))\n",
    "    print(f\"‚úì Found {num_images} images\")\n",
    "else:\n",
    "    print(\"‚ö† Images directory not found. Make sure you ran the setup cells above!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f44b72",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load BLIP-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78ac686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and initialize BLIP-2 cross-encoder\n",
    "try:\n",
    "    from src.retrieval.cross_encoder import CrossEncoder\n",
    "except ImportError:\n",
    "    # Add project root if not already in path\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "    project_root = Path.cwd() if Path.cwd().name != 'notebooks' else Path.cwd().parent\n",
    "    if project_root not in sys.path:\n",
    "        sys.path.insert(0, str(project_root))\n",
    "    from src.retrieval.cross_encoder import CrossEncoder\n",
    "\n",
    "print(\"Loading BLIP-2 from Hugging Face...\")\n",
    "print(\"Model: Salesforce/blip2-opt-2.7b (~3GB with FP16, fits P100 16GB GPU alongside CLIP)\")\n",
    "\n",
    "encoder = CrossEncoder(\n",
    "    model_name='Salesforce/blip2-opt-2.7b',\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    use_fp16=True  # Important for GPU memory efficiency\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì BLIP-2 model loaded successfully!\")\n",
    "print(f\"Device: {encoder.device}\")\n",
    "print(f\"FP16: {encoder.use_fp16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7fc758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model information\n",
    "print(\"Model Information:\")\n",
    "print(f\"  Model class: {encoder.model.__class__.__name__}\")\n",
    "print(f\"  Device: {encoder.device}\")\n",
    "print(f\"  FP16 enabled: {encoder.use_fp16}\")\n",
    "print(f\"  Default batch size: {encoder.batch_size}\")\n",
    "print(f\"  Max batch size: {encoder.max_batch_size}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU Memory Usage:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "    print(f\"  Reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0972c83e",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Test Single Pair Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d56d794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a test image\n",
    "if IMAGES_DIR.exists():\n",
    "    test_images = list(IMAGES_DIR.glob('*.jpg'))[:5]\n",
    "    \n",
    "    if len(test_images) > 0:\n",
    "        test_image_path = test_images[0]\n",
    "        \n",
    "        # Display the image\n",
    "        img = Image.open(test_image_path)\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Test Image: {test_image_path.name}\", fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Image: {test_image_path.name}\")\n",
    "        print(f\"Size: {img.size}\")\n",
    "    else:\n",
    "        print(\"No images found in directory\")\n",
    "else:\n",
    "    print(\"Images directory not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecba432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different queries\n",
    "test_queries = [\n",
    "    \"A dog playing in the park\",\n",
    "    \"People at a beach\",\n",
    "    \"A colorful outdoor scene\",\n",
    "    \"Children playing together\",\n",
    "    \"Random unrelated text xyz123\",\n",
    "]\n",
    "\n",
    "print(f\"Testing single pair scoring\\n\")\n",
    "print(f\"Image: {test_image_path.name}\\n\")\n",
    "print(\"Query ‚Üí Score\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "scores_dict = {}\n",
    "for query in test_queries:\n",
    "    score = encoder.score_pair(query, test_image_path)\n",
    "    scores_dict[query] = score\n",
    "    print(f\"{query:40s} ‚Üí {score:.4f}\")\n",
    "\n",
    "print(\"\\n‚úì Single pair scoring works!\")\n",
    "print(f\"Score range: [{min(scores_dict.values()):.4f}, {max(scores_dict.values()):.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b146e71",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Batch Processing Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d431f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "n_pairs = 10\n",
    "batch_test_images = list(IMAGES_DIR.glob('*.jpg'))[:n_pairs]\n",
    "batch_test_queries = [\n",
    "    \"A photograph of people\",\n",
    "    \"An outdoor scene\",\n",
    "    \"Children playing\",\n",
    "    \"A colorful image\",\n",
    "    \"An action scene\",\n",
    "    \"A landscape view\",\n",
    "    \"People in a setting\",\n",
    "    \"An indoor environment\",\n",
    "    \"A busy scene\",\n",
    "    \"A peaceful moment\"\n",
    "][:n_pairs]\n",
    "\n",
    "print(f\"Prepared {len(batch_test_images)} image-query pairs for batch testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253d593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different batch sizes\n",
    "batch_sizes = [1, 2, 4, 8]\n",
    "results = []\n",
    "\n",
    "print(\"Testing different batch sizes...\\n\")\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    \n",
    "    start = time.time()\n",
    "    scores = encoder.score_pairs(\n",
    "        batch_test_queries,\n",
    "        batch_test_images,\n",
    "        batch_size=batch_size,\n",
    "        show_progress=True\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    results.append({\n",
    "        'batch_size': batch_size,\n",
    "        'total_time': elapsed,\n",
    "        'time_per_pair': elapsed / len(batch_test_queries),\n",
    "        'mean_score': scores.mean(),\n",
    "        'std_score': scores.std()\n",
    "    })\n",
    "    \n",
    "    print(f\"  Time: {elapsed:.2f}s ({elapsed/len(batch_test_queries)*1000:.1f}ms per pair)\")\n",
    "    print(f\"  Mean score: {scores.mean():.4f} ¬± {scores.std():.4f}\\n\")\n",
    "\n",
    "# Display results table\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Batch Size Performance Comparison\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\n‚úì Optimal batch size: \" + str(results_df.loc[results_df['time_per_pair'].idxmin(), 'batch_size']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfae4567",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ CLIP vs BLIP-2 Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0024c0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP bi-encoder for comparison\n",
    "try:\n",
    "    from src.retrieval.bi_encoder import BiEncoder\n",
    "except ImportError:\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "    project_root = Path.cwd() if Path.cwd().name != 'notebooks' else Path.cwd().parent\n",
    "    if project_root not in sys.path:\n",
    "        sys.path.insert(0, str(project_root))\n",
    "    from src.retrieval.bi_encoder import BiEncoder\n",
    "\n",
    "print(\"Loading CLIP bi-encoder...\")\n",
    "clip_encoder = BiEncoder(model_name='ViT-B-32', pretrained='openai')\n",
    "print(\"‚úì CLIP loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a725c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare on same pairs\n",
    "comparison_queries = batch_test_queries[:5]\n",
    "comparison_images = batch_test_images[:5]\n",
    "\n",
    "print(\"Computing CLIP scores...\")\n",
    "clip_img_embs = clip_encoder.encode_images(comparison_images, show_progress=False)\n",
    "clip_text_embs = clip_encoder.encode_texts(comparison_queries, show_progress=False)\n",
    "clip_scores = (clip_img_embs * clip_text_embs).sum(axis=1)\n",
    "\n",
    "print(\"Computing BLIP-2 scores...\")\n",
    "blip2_scores = encoder.score_pairs(\n",
    "    queries=comparison_queries,\n",
    "    candidates=comparison_images,\n",
    "    query_type='text',\n",
    "    candidate_type='image',\n",
    "    batch_size=4,\n",
    "    show_progress=False\n",
    ")\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Query': [q[:30] + '...' if len(q) > 30 else q for q in comparison_queries],\n",
    "    'Image': [img.name for img in comparison_images],\n",
    "    'CLIP': clip_scores,\n",
    "    'BLIP-2': blip2_scores,\n",
    "    'Diff': blip2_scores - clip_scores\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLIP (Bi-Encoder) vs BLIP-2 (Cross-Encoder) Comparison\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e12100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart comparison\n",
    "x = np.arange(len(clip_scores))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, clip_scores, width, label='CLIP', alpha=0.8)\n",
    "axes[0].bar(x + width/2, blip2_scores, width, label='BLIP-2', alpha=0.8)\n",
    "axes[0].set_xlabel('Query Index', fontsize=12)\n",
    "axes[0].set_ylabel('Score', fontsize=12)\n",
    "axes[0].set_title('CLIP vs BLIP-2 Scores', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot correlation\n",
    "axes[1].scatter(clip_scores, blip2_scores, alpha=0.7, s=100, edgecolors='black')\n",
    "axes[1].plot([0, 1], [0, 1], 'r--', alpha=0.5, linewidth=2, label='y=x')\n",
    "axes[1].set_xlabel('CLIP Score', fontsize=12)\n",
    "axes[1].set_ylabel('BLIP-2 Score', fontsize=12)\n",
    "axes[1].set_title('Score Correlation', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute correlation\n",
    "correlation = np.corrcoef(clip_scores, blip2_scores)[0, 1]\n",
    "print(f\"\\nCorrelation between CLIP and BLIP-2: {correlation:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642a4801",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Performance Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff6eaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: Score 100 pairs (simulating reranking top-100)\n",
    "n_benchmark = 100\n",
    "available_images = list(IMAGES_DIR.glob('*.jpg'))\n",
    "\n",
    "if len(available_images) < n_benchmark:\n",
    "    n_benchmark = len(available_images)\n",
    "    print(f\"‚ö† Only {n_benchmark} images available\")\n",
    "\n",
    "benchmark_images = available_images[:n_benchmark]\n",
    "benchmark_queries = [\"A photograph\"] * n_benchmark  # Same query for all\n",
    "\n",
    "print(f\"Benchmarking BLIP-2 with {n_benchmark} pairs...\")\n",
    "print(\"This simulates reranking top-100 bi-encoder results\\n\")\n",
    "\n",
    "start = time.time()\n",
    "scores = encoder.score_pairs(\n",
    "    queries=benchmark_queries,\n",
    "    candidates=benchmark_images,\n",
    "    query_type='text',\n",
    "    candidate_type='image',\n",
    "    batch_size=8,\n",
    "    show_progress=True\n",
    ")\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"\\n\" + \"=\"*70)\n",
    "print(\"Benchmark Results\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total time:        {elapsed:.2f}s\")\n",
    "print(f\"Time per pair:     {elapsed/n_benchmark*1000:.1f}ms\")\n",
    "print(f\"Throughput:        {n_benchmark/elapsed:.1f} pairs/second\")\n",
    "print(f\"\\nTarget (Week 1):   < 30 seconds for 100 pairs\")\n",
    "print(f\"Status:            {'‚úì PASS' if elapsed < 30 else '‚ö† SLOW'}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9806a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize score distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(scores, bins=30, alpha=0.7, edgecolor='black', color='steelblue')\n",
    "axes[0].axvline(scores.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {scores.mean():.3f}')\n",
    "axes[0].axvline(np.median(scores), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(scores):.3f}')\n",
    "axes[0].set_xlabel('BLIP-2 Score', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title(f'Score Distribution (n={n_benchmark})', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(scores, vert=True, patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                medianprops=dict(color='red', linewidth=2))\n",
    "axes[1].set_ylabel('BLIP-2 Score', fontsize=12)\n",
    "axes[1].set_title('Score Statistics', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nScore Statistics:\")\n",
    "print(f\"  Mean:    {scores.mean():.4f}\")\n",
    "print(f\"  Median:  {np.median(scores):.4f}\")\n",
    "print(f\"  Std:     {scores.std():.4f}\")\n",
    "print(f\"  Min:     {scores.min():.4f}\")\n",
    "print(f\"  Max:     {scores.max():.4f}\")\n",
    "print(f\"  Range:   {scores.max() - scores.min():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43f8281",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab89800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with diverse queries on the same image\n",
    "diverse_queries = [\n",
    "    \"people standing together\",\n",
    "    \"outdoor sunny day\",\n",
    "    \"children playing\",\n",
    "    \"buildings and architecture\",\n",
    "    \"animals in nature\",\n",
    "    \"food on a table\",\n",
    "    \"sports activity\",\n",
    "    \"night scene with lights\",\n",
    "]\n",
    "\n",
    "print(f\"Testing query diversity on: {test_image_path.name}\\n\")\n",
    "\n",
    "diverse_scores = []\n",
    "for query in diverse_queries:\n",
    "    score = encoder.score_pair(query, test_image_path)\n",
    "    diverse_scores.append(score)\n",
    "    print(f\"{query:30s} ‚Üí {score:.4f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(diverse_queries, diverse_scores, color='steelblue', alpha=0.8, edgecolor='black')\n",
    "plt.xlabel('BLIP-2 Score', fontsize=12)\n",
    "plt.title(f'Query Diversity Test - {test_image_path.name}', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Score range: [{min(diverse_scores):.4f}, {max(diverse_scores):.4f}]\")\n",
    "print(f\"Score variance: {np.var(diverse_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31d73f9",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322b0087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "summary = f\"\"\"\n",
    "{'='*70}\n",
    "PHASE 3 WEEK 1: BLIP-2 EXPLORATION SUMMARY\n",
    "{'='*70}\n",
    "\n",
    "‚úì ACCOMPLISHMENTS:\n",
    "  ‚Ä¢ BLIP-2 successfully loaded from Hugging Face\n",
    "  \"  ‚Ä¢ Model: Salesforce/blip2-opt-2.7b (~3GB with FP16, fits P100 alongside CLIP)\\n\",\n",
    "  ‚Ä¢ Device: {encoder.device}\n",
    "  ‚Ä¢ FP16 enabled: {encoder.use_fp16}\n",
    "  \n",
    "‚úì PERFORMANCE:\n",
    "  ‚Ä¢ Single pair scoring: ‚úì Working\n",
    "  ‚Ä¢ Batch processing: ‚úì Optimized\n",
    "  ‚Ä¢ Benchmark ({n_benchmark} pairs): {elapsed:.2f}s\n",
    "  ‚Ä¢ Throughput: {n_benchmark/elapsed:.1f} pairs/second\n",
    "  ‚Ä¢ Target status: {'‚úì PASS' if elapsed < 30 else '‚ö† NEEDS OPTIMIZATION'}\n",
    "\n",
    "‚úì QUALITY:\n",
    "  ‚Ä¢ Score range: [0, 1] (interpretable)\n",
    "  ‚Ä¢ Uses yes/no probability method\n",
    "  ‚Ä¢ Different perspective than CLIP\n",
    "  ‚Ä¢ Correlation with CLIP: {correlation:.3f}\n",
    "\n",
    "üìã WEEK 2 TASKS (Nov 4-30):\n",
    "  1. Implement reranking function\n",
    "  2. Create HybridRetriever class\n",
    "  3. Integrate bi-encoder + cross-encoder pipeline\n",
    "  4. Implement evaluation metrics (Recall@K, Precision@K, MRR)\n",
    "  5. Build demo notebook for hybrid retrieval\n",
    "  6. Compare hybrid vs bi-encoder performance\n",
    "\n",
    "üéØ SUCCESS CRITERIA FOR WEEK 2:\n",
    "  ‚Ä¢ Reranking time < 30 seconds for top-100\n",
    "  ‚Ä¢ Recall@10 improvement: +15-20% over bi-encoder\n",
    "  ‚Ä¢ Precision@10 > 60%\n",
    "  ‚Ä¢ Full hybrid pipeline working end-to-end\n",
    "\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6383d9ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Checkpoint\n",
    "\n",
    "**Week 1 Complete!** All BLIP-2 integration tests passed.\n",
    "\n",
    "Next: Proceed to Week 2 - Hybrid Retrieval Pipeline"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
