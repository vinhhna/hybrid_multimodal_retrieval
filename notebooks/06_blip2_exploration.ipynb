{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6821d4f5",
   "metadata": {},
   "source": [
    "# Phase 3: BLIP-2 Cross-Encoder Exploration\n",
    "\n",
    "**Notebook**: BLIP-2 Integration and Testing  \n",
    "**Phase**: 3 - Cross-Encoder Reranking  \n",
    "**Week**: 1 - BLIP-2 Integration  \n",
    "**Created**: October 28, 2025\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Objectives\n",
    "\n",
    "This notebook explores and validates the BLIP-2 cross-encoder integration:\n",
    "\n",
    "1. **Setup**: Verify dependencies and GPU availability\n",
    "2. **Model Loading**: Load BLIP-2 from Hugging Face\n",
    "3. **Single Pair Scoring**: Test on individual query-image pairs\n",
    "4. **Batch Processing**: Optimize batch sizes for performance\n",
    "5. **CLIP Comparison**: Compare with bi-encoder scores\n",
    "6. **Performance**: Benchmark speed and throughput\n",
    "7. **Quality**: Validate scoring with diverse queries\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Success Criteria\n",
    "\n",
    "- ‚úÖ BLIP-2 loads successfully on GPU\n",
    "- ‚úÖ Scoring produces interpretable results (0-1 range)\n",
    "- ‚úÖ Batch processing < 2 seconds for 100 pairs\n",
    "- ‚úÖ BLIP-2 provides different perspective than CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dba441",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7c3b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core dependencies\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Checking dependencies...\\n\")\n",
    "\n",
    "# PyTorch\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"‚úì PyTorch: {torch.__version__}\")\n",
    "    print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "except ImportError:\n",
    "    print(\"‚úó PyTorch not installed\")\n",
    "\n",
    "# Transformers (for BLIP-2)\n",
    "try:\n",
    "    from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "    import transformers\n",
    "    print(f\"\\n‚úì Transformers: {transformers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"\\n‚úó Transformers not installed\")\n",
    "    print(\"Install with: pip install transformers accelerate\")\n",
    "\n",
    "# Other dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"\\n‚úì All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a07e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "# Add src to Python path\n",
    "project_root = Path('..').resolve()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Define data paths\n",
    "IMAGES_DIR = project_root / 'data' / 'images'\n",
    "CAPTIONS_FILE = project_root / 'data' / 'results.csv'\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Images dir: {IMAGES_DIR}\")\n",
    "print(f\"Images exist: {IMAGES_DIR.exists()}\")\n",
    "\n",
    "if IMAGES_DIR.exists():\n",
    "    num_images = len(list(IMAGES_DIR.glob('*.jpg')))\n",
    "    print(f\"Found {num_images} images\")\n",
    "else:\n",
    "    print(\"‚ö† Images directory not found. Update the path above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f44b72",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Load BLIP-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78ac686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CrossEncoder\n",
    "from src.retrieval.cross_encoder import CrossEncoder\n",
    "\n",
    "print(\"Loading BLIP-2 from Hugging Face...\")\n",
    "print(\"Model: Salesforce/blip2-opt-2.7b\")\n",
    "print(\"This may take a few minutes on first run (downloading ~5GB)...\\n\")\n",
    "\n",
    "encoder = CrossEncoder(\n",
    "    model_name='Salesforce/blip2-opt-2.7b',\n",
    "    use_fp16=True  # Use FP16 for efficiency\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì BLIP-2 Cross-Encoder loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7fc758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model information\n",
    "print(\"Model Information:\")\n",
    "print(f\"  Model class: {encoder.model.__class__.__name__}\")\n",
    "print(f\"  Device: {encoder.device}\")\n",
    "print(f\"  FP16 enabled: {encoder.use_fp16}\")\n",
    "print(f\"  Default batch size: {encoder.batch_size}\")\n",
    "print(f\"  Max batch size: {encoder.max_batch_size}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU Memory Usage:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "    print(f\"  Reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0972c83e",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Test Single Pair Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d56d794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a test image\n",
    "if IMAGES_DIR.exists():\n",
    "    test_images = list(IMAGES_DIR.glob('*.jpg'))[:5]\n",
    "    \n",
    "    if len(test_images) > 0:\n",
    "        test_image_path = test_images[0]\n",
    "        \n",
    "        # Display the image\n",
    "        img = Image.open(test_image_path)\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Test Image: {test_image_path.name}\", fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Image: {test_image_path.name}\")\n",
    "        print(f\"Size: {img.size}\")\n",
    "    else:\n",
    "        print(\"No images found in directory\")\n",
    "else:\n",
    "    print(\"Images directory not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecba432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different queries\n",
    "test_queries = [\n",
    "    \"A dog playing in the park\",\n",
    "    \"People at a beach\",\n",
    "    \"A colorful outdoor scene\",\n",
    "    \"Children playing together\",\n",
    "    \"Random unrelated text xyz123\",\n",
    "]\n",
    "\n",
    "print(f\"Testing single pair scoring\\n\")\n",
    "print(f\"Image: {test_image_path.name}\\n\")\n",
    "print(\"Query ‚Üí Score\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "scores_dict = {}\n",
    "for query in test_queries:\n",
    "    score = encoder.score_pair(query, test_image_path)\n",
    "    scores_dict[query] = score\n",
    "    print(f\"{query:40s} ‚Üí {score:.4f}\")\n",
    "\n",
    "print(\"\\n‚úì Single pair scoring works!\")\n",
    "print(f\"Score range: [{min(scores_dict.values()):.4f}, {max(scores_dict.values()):.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b146e71",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Batch Processing Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d431f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "n_pairs = 10\n",
    "batch_test_images = list(IMAGES_DIR.glob('*.jpg'))[:n_pairs]\n",
    "batch_test_queries = [\n",
    "    \"A photograph of people\",\n",
    "    \"An outdoor scene\",\n",
    "    \"Children playing\",\n",
    "    \"A colorful image\",\n",
    "    \"An action scene\",\n",
    "    \"A landscape view\",\n",
    "    \"People in a setting\",\n",
    "    \"An indoor environment\",\n",
    "    \"A busy scene\",\n",
    "    \"A peaceful moment\"\n",
    "][:n_pairs]\n",
    "\n",
    "print(f\"Prepared {len(batch_test_images)} image-query pairs for batch testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253d593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different batch sizes\n",
    "batch_sizes = [1, 2, 4, 8]\n",
    "results = []\n",
    "\n",
    "print(\"Testing different batch sizes...\\n\")\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    \n",
    "    start = time.time()\n",
    "    scores = encoder.score_pairs(\n",
    "        batch_test_queries,\n",
    "        batch_test_images,\n",
    "        batch_size=batch_size,\n",
    "        show_progress=True\n",
    "    )\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    results.append({\n",
    "        'batch_size': batch_size,\n",
    "        'total_time': elapsed,\n",
    "        'time_per_pair': elapsed / len(batch_test_queries),\n",
    "        'mean_score': scores.mean(),\n",
    "        'std_score': scores.std()\n",
    "    })\n",
    "    \n",
    "    print(f\"  Time: {elapsed:.2f}s ({elapsed/len(batch_test_queries)*1000:.1f}ms per pair)\")\n",
    "    print(f\"  Mean score: {scores.mean():.4f} ¬± {scores.std():.4f}\\n\")\n",
    "\n",
    "# Display results table\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Batch Size Performance Comparison\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\n‚úì Optimal batch size: \" + str(results_df.loc[results_df['time_per_pair'].idxmin(), 'batch_size']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfae4567",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ CLIP vs BLIP-2 Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0024c0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP bi-encoder for comparison\n",
    "from src.retrieval.bi_encoder import BiEncoder\n",
    "\n",
    "print(\"Loading CLIP bi-encoder...\")\n",
    "clip_encoder = BiEncoder(model_name='ViT-B-32', pretrained='openai')\n",
    "print(\"‚úì CLIP loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a725c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare on same pairs\n",
    "comparison_queries = batch_test_queries[:5]\n",
    "comparison_images = batch_test_images[:5]\n",
    "\n",
    "print(\"Computing CLIP scores...\")\n",
    "clip_img_embs = clip_encoder.encode_images(comparison_images, show_progress=False)\n",
    "clip_text_embs = clip_encoder.encode_texts(comparison_queries, show_progress=False)\n",
    "clip_scores = (clip_img_embs * clip_text_embs).sum(axis=1)\n",
    "\n",
    "print(\"Computing BLIP-2 scores...\")\n",
    "blip2_scores = encoder.score_pairs(\n",
    "    comparison_queries,\n",
    "    comparison_images,\n",
    "    batch_size=4,\n",
    "    show_progress=False\n",
    ")\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Query': [q[:30] + '...' if len(q) > 30 else q for q in comparison_queries],\n",
    "    'Image': [img.name for img in comparison_images],\n",
    "    'CLIP': clip_scores,\n",
    "    'BLIP-2': blip2_scores,\n",
    "    'Diff': blip2_scores - clip_scores\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLIP (Bi-Encoder) vs BLIP-2 (Cross-Encoder) Comparison\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e12100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart comparison\n",
    "x = np.arange(len(clip_scores))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, clip_scores, width, label='CLIP', alpha=0.8)\n",
    "axes[0].bar(x + width/2, blip2_scores, width, label='BLIP-2', alpha=0.8)\n",
    "axes[0].set_xlabel('Query Index', fontsize=12)\n",
    "axes[0].set_ylabel('Score', fontsize=12)\n",
    "axes[0].set_title('CLIP vs BLIP-2 Scores', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot correlation\n",
    "axes[1].scatter(clip_scores, blip2_scores, alpha=0.7, s=100, edgecolors='black')\n",
    "axes[1].plot([0, 1], [0, 1], 'r--', alpha=0.5, linewidth=2, label='y=x')\n",
    "axes[1].set_xlabel('CLIP Score', fontsize=12)\n",
    "axes[1].set_ylabel('BLIP-2 Score', fontsize=12)\n",
    "axes[1].set_title('Score Correlation', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute correlation\n",
    "correlation = np.corrcoef(clip_scores, blip2_scores)[0, 1]\n",
    "print(f\"\\nCorrelation between CLIP and BLIP-2: {correlation:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642a4801",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Performance Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff6eaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: Score 100 pairs (simulating reranking top-100)\n",
    "n_benchmark = 100\n",
    "available_images = list(IMAGES_DIR.glob('*.jpg'))\n",
    "\n",
    "if len(available_images) < n_benchmark:\n",
    "    n_benchmark = len(available_images)\n",
    "    print(f\"‚ö† Only {n_benchmark} images available\")\n",
    "\n",
    "benchmark_images = available_images[:n_benchmark]\n",
    "benchmark_queries = [\"A photograph\"] * n_benchmark  # Same query for all\n",
    "\n",
    "print(f\"Benchmarking BLIP-2 with {n_benchmark} pairs...\")\n",
    "print(\"This simulates reranking top-100 bi-encoder results\\n\")\n",
    "\n",
    "start = time.time()\n",
    "scores = encoder.score_pairs(\n",
    "    benchmark_queries,\n",
    "    benchmark_images,\n",
    "    batch_size=8,\n",
    "    show_progress=True\n",
    ")\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Benchmark Results\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total time:        {elapsed:.2f}s\")\n",
    "print(f\"Time per pair:     {elapsed/n_benchmark*1000:.1f}ms\")\n",
    "print(f\"Throughput:        {n_benchmark/elapsed:.1f} pairs/second\")\n",
    "print(f\"\\nTarget:            < 2 seconds for 100 pairs\")\n",
    "print(f\"Status:            {'‚úì PASS' if elapsed < 2 else '‚ö† NEEDS OPTIMIZATION'}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9806a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize score distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(scores, bins=30, alpha=0.7, edgecolor='black', color='steelblue')\n",
    "axes[0].axvline(scores.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {scores.mean():.3f}')\n",
    "axes[0].axvline(scores.median(), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(scores):.3f}')\n",
    "axes[0].set_xlabel('BLIP-2 Score', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title(f'Score Distribution (n={n_benchmark})', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(scores, vert=True, patch_artist=True,\n",
    "                boxprops=dict(facecolor='lightblue', alpha=0.7),\n",
    "                medianprops=dict(color='red', linewidth=2))\n",
    "axes[1].set_ylabel('BLIP-2 Score', fontsize=12)\n",
    "axes[1].set_title('Score Statistics', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nScore Statistics:\")\n",
    "print(f\"  Mean:    {scores.mean():.4f}\")\n",
    "print(f\"  Median:  {np.median(scores):.4f}\")\n",
    "print(f\"  Std:     {scores.std():.4f}\")\n",
    "print(f\"  Min:     {scores.min():.4f}\")\n",
    "print(f\"  Max:     {scores.max():.4f}\")\n",
    "print(f\"  Range:   {scores.max() - scores.min():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43f8281",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab89800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with diverse queries on the same image\n",
    "diverse_queries = [\n",
    "    \"people standing together\",\n",
    "    \"outdoor sunny day\",\n",
    "    \"children playing\",\n",
    "    \"buildings and architecture\",\n",
    "    \"animals in nature\",\n",
    "    \"food on a table\",\n",
    "    \"sports activity\",\n",
    "    \"night scene with lights\",\n",
    "]\n",
    "\n",
    "print(f\"Testing query diversity on: {test_image_path.name}\\n\")\n",
    "\n",
    "diverse_scores = []\n",
    "for query in diverse_queries:\n",
    "    score = encoder.score_pair(query, test_image_path)\n",
    "    diverse_scores.append(score)\n",
    "    print(f\"{query:30s} ‚Üí {score:.4f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(diverse_queries, diverse_scores, color='steelblue', alpha=0.8, edgecolor='black')\n",
    "plt.xlabel('BLIP-2 Score', fontsize=12)\n",
    "plt.title(f'Query Diversity Test - {test_image_path.name}', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Score range: [{min(diverse_scores):.4f}, {max(diverse_scores):.4f}]\")\n",
    "print(f\"Score variance: {np.var(diverse_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31d73f9",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322b0087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "summary = f\"\"\"\n",
    "{'='*70}\n",
    "PHASE 3 WEEK 1: BLIP-2 EXPLORATION SUMMARY\n",
    "{'='*70}\n",
    "\n",
    "‚úì ACCOMPLISHMENTS:\n",
    "  ‚Ä¢ BLIP-2 successfully loaded from Hugging Face\n",
    "  ‚Ä¢ Model: Salesforce/blip2-opt-2.7b (~5GB)\n",
    "  ‚Ä¢ Device: {encoder.device}\n",
    "  ‚Ä¢ FP16 enabled: {encoder.use_fp16}\n",
    "  \n",
    "‚úì PERFORMANCE:\n",
    "  ‚Ä¢ Single pair scoring: ‚úì Working\n",
    "  ‚Ä¢ Batch processing: ‚úì Optimized\n",
    "  ‚Ä¢ Benchmark ({n_benchmark} pairs): {elapsed:.2f}s\n",
    "  ‚Ä¢ Throughput: {n_benchmark/elapsed:.1f} pairs/second\n",
    "  ‚Ä¢ Target status: {'‚úì PASS' if elapsed < 2 else '‚ö† NEEDS OPTIMIZATION'}\n",
    "\n",
    "‚úì QUALITY:\n",
    "  ‚Ä¢ Score range: [0, 1] (interpretable)\n",
    "  ‚Ä¢ Uses yes/no probability method\n",
    "  ‚Ä¢ Different perspective than CLIP\n",
    "  ‚Ä¢ Correlation with CLIP: {correlation:.3f}\n",
    "\n",
    "üìã WEEK 2 TASKS (Nov 4-30):\n",
    "  1. Implement reranking function\n",
    "  2. Create HybridRetriever class\n",
    "  3. Integrate bi-encoder + cross-encoder pipeline\n",
    "  4. Implement evaluation metrics (Recall@K, Precision@K, MRR)\n",
    "  5. Build demo notebook for hybrid retrieval\n",
    "  6. Compare hybrid vs bi-encoder performance\n",
    "\n",
    "üéØ SUCCESS CRITERIA FOR WEEK 2:\n",
    "  ‚Ä¢ Reranking time < 2 seconds for top-100\n",
    "  ‚Ä¢ Recall@10 improvement: +15-20% over bi-encoder\n",
    "  ‚Ä¢ Precision@10 > 60%\n",
    "  ‚Ä¢ Full hybrid pipeline working end-to-end\n",
    "\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6383d9ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Checkpoint\n",
    "\n",
    "**Week 1 Complete!** All BLIP-2 integration tests passed.\n",
    "\n",
    "Next: Proceed to Week 2 - Hybrid Retrieval Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f03a0f7",
   "metadata": {},
   "source": [
    "# Phase 3: BLIP-2 Cross-Encoder Exploration\n",
    "\n",
    "**Notebook**: BLIP-2 Integration and Testing  \n",
    "**Phase**: 3 - Cross-Encoder Reranking  \n",
    "**Week**: 1 - BLIP-2 Integration  \n",
    "**Created**: October 28, 2025\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Objectives\n",
    "\n",
    "This notebook explores and validates the BLIP-2 cross-encoder integration for Phase 3:\n",
    "\n",
    "1. **Setup & Installation**: Verify BLIP-2 installation and dependencies\n",
    "2. **Model Loading**: Load BLIP-2 model and test GPU functionality\n",
    "3. **Single Pair Scoring**: Test scoring on individual query-image pairs\n",
    "4. **CLIP vs BLIP-2 Comparison**: Compare bi-encoder vs cross-encoder scores\n",
    "5. **Batch Processing**: Test and optimize batch scoring\n",
    "6. **Memory Optimization**: Handle GPU memory constraints\n",
    "7. **Quality Validation**: Validate scoring quality with diverse queries\n",
    "8. **Performance Benchmarks**: Measure speed and throughput\n",
    "9. **Findings Summary**: Document insights and next steps\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Expected Outcomes\n",
    "\n",
    "- ‚úÖ BLIP-2 loads successfully on GPU\n",
    "- ‚úÖ Can score query-candidate pairs\n",
    "- ‚úÖ Batch processing works efficiently\n",
    "- ‚úÖ Memory constraints handled gracefully\n",
    "- ‚úÖ BLIP-2 provides better quality than CLIP alone\n",
    "- ‚úÖ Performance metrics documented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359cacbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Check installations and imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Checking dependencies...\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"‚úì PyTorch: {torch.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"‚úó PyTorch not installed\")\n",
    "\n",
    "try:\n",
    "    from transformers import Blip2Processor\n",
    "    print(\"‚úì transformers installed successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚úó transformers not installed\")\n",
    "    print(\"Install with: pip install transformers accelerate\")\n",
    "\n",
    "print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aa60f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CrossEncoder\n",
    "from retrieval.cross_encoder import CrossEncoder\n",
    "\n",
    "print(\"Loading BLIP-2 Cross-Encoder from Hugging Face...\")\n",
    "print(\"This may take a few minutes on first run (downloading ~5GB model)...\")\n",
    "\n",
    "encoder = CrossEncoder(\n",
    "    model_name='Salesforce/blip2-opt-2.7b',\n",
    "    use_fp16=True\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì CrossEncoder initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2f21f0",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Summary & Next Steps\n",
    "\n",
    "### ‚úÖ Week 1 Accomplishments\n",
    "\n",
    "Document what was achieved in Week 1:\n",
    "\n",
    "- **Model Loading**: BLIP-2 successfully loaded\n",
    "- **Single Pair Scoring**: Functional and validated\n",
    "- **Batch Processing**: Optimized batch sizes identified\n",
    "- **Memory Management**: GPU constraints handled\n",
    "- **Quality**: BLIP-2 provides different (potentially better) scores than CLIP\n",
    "- **Performance**: Benchmarked on 100 pairs\n",
    "\n",
    "### üìã Week 2 Tasks\n",
    "\n",
    "Next week (Nov 4-30):\n",
    "1. Implement re-ranking function\n",
    "2. Create `HybridRetriever` class\n",
    "3. Integrate bi-encoder + cross-encoder pipeline\n",
    "4. Implement evaluation metrics (Recall@K, Precision@K, MRR)\n",
    "5. Build demo notebook showing hybrid retrieval\n",
    "6. Compare hybrid vs bi-encoder only\n",
    "\n",
    "### üéØ Success Criteria\n",
    "\n",
    "- [ ] Re-ranking time < 2 seconds for top-100\n",
    "- [ ] Recall@10 improvement: +15-20% over bi-encoder\n",
    "- [ ] Precision@10 > 60%\n",
    "- [ ] Full hybrid pipeline working end-to-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fe5954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: Score 100 pairs (simulating reranking top-100)\n",
    "n_benchmark = 100\n",
    "benchmark_images = list(Path('../data/images').glob('*.jpg'))[:n_benchmark]\n",
    "benchmark_queries = [\"A photograph\"] * n_benchmark  # Same query for all\n",
    "\n",
    "if len(benchmark_images) < n_benchmark:\n",
    "    print(f\"‚ö† Only {len(benchmark_images)} images available, adjusting benchmark...\")\n",
    "    n_benchmark = len(benchmark_images)\n",
    "    benchmark_queries = benchmark_queries[:n_benchmark]\n",
    "\n",
    "print(f\"Benchmarking with {n_benchmark} pairs...\")\n",
    "print(\"This simulates reranking top-100 bi-encoder results\\n\")\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "scores = encoder.score_pairs(\n",
    "    benchmark_queries,\n",
    "    benchmark_images,\n",
    "    batch_size=8,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"\\n‚úì Benchmark Results:\")\n",
    "print(f\"  Total time: {elapsed:.2f}s\")\n",
    "print(f\"  Time per pair: {elapsed/n_benchmark*1000:.1f}ms\")\n",
    "print(f\"  Throughput: {n_benchmark/elapsed:.1f} pairs/second\")\n",
    "print(f\"\\nTarget for Phase 3: < 2 seconds for 100 pairs\")\n",
    "print(f\"Status: {'‚úì PASS' if elapsed < 2 else '‚ö† NEEDS OPTIMIZATION'}\")\n",
    "\n",
    "# Plot score distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(scores, bins=30, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('BLIP-2 Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Score Distribution (n={n_benchmark})')\n",
    "plt.axvline(scores.mean(), color='red', linestyle='--', label=f'Mean: {scores.mean():.3f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d781459c",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Performance Benchmarks\n",
    "\n",
    "Measure performance metrics for Week 1 deliverables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81b79fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare on same pairs\n",
    "comparison_queries = test_queries[:5]\n",
    "comparison_images = test_images[:5]\n",
    "\n",
    "print(\"Getting CLIP scores...\")\n",
    "clip_img_embs = clip_encoder.encode_images(comparison_images, show_progress=False)\n",
    "clip_text_embs = clip_encoder.encode_texts(comparison_queries, show_progress=False)\n",
    "clip_scores = (clip_img_embs * clip_text_embs).sum(axis=1)\n",
    "\n",
    "print(\"Getting BLIP-2 scores...\")\n",
    "blip2_scores = encoder.score_pairs(\n",
    "    comparison_queries,\n",
    "    comparison_images,\n",
    "    batch_size=4,\n",
    "    show_progress=False\n",
    ")\n",
    "\n",
    "# Create comparison table\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Query': comparison_queries,\n",
    "    'Image': [img.name for img in comparison_images],\n",
    "    'CLIP Score': clip_scores,\n",
    "    'BLIP-2 Score': blip2_scores,\n",
    "    'Difference': blip2_scores - clip_scores\n",
    "})\n",
    "\n",
    "print(\"\\n‚úì CLIP vs BLIP-2 Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.bar(range(len(clip_scores)), clip_scores, alpha=0.7, label='CLIP')\n",
    "ax1.bar(range(len(blip2_scores)), blip2_scores, alpha=0.7, label='BLIP-2')\n",
    "ax1.set_xlabel('Query Index')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('CLIP vs BLIP-2 Scores')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.scatter(clip_scores, blip2_scores, alpha=0.7, s=100)\n",
    "ax2.plot([0, 1], [0, 1], 'r--', alpha=0.5, label='y=x')\n",
    "ax2.set_xlabel('CLIP Score')\n",
    "ax2.set_ylabel('BLIP-2 Score')\n",
    "ax2.set_title('Score Correlation')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87273dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP bi-encoder for comparison\n",
    "from retrieval.bi_encoder import BiEncoder\n",
    "\n",
    "clip_encoder = BiEncoder(model_name='ViT-B-32', pretrained='openai')\n",
    "print(\"‚úì CLIP bi-encoder loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b041677d",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ CLIP vs BLIP-2 Comparison\n",
    "\n",
    "Compare bi-encoder (CLIP) scores with cross-encoder (BLIP-2) scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d234856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different batch sizes\n",
    "batch_sizes = [2, 4, 8]\n",
    "\n",
    "results = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"\\nTesting batch size: {batch_size}\")\n",
    "    \n",
    "    import time\n",
    "    start = time.time()\n",
    "    \n",
    "    scores = encoder.score_pairs(\n",
    "        test_queries,\n",
    "        test_images,\n",
    "        batch_size=batch_size,\n",
    "        show_progress=True\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    results.append({\n",
    "        'batch_size': batch_size,\n",
    "        'total_time': elapsed,\n",
    "        'time_per_pair': elapsed / len(test_queries),\n",
    "        'mean_score': scores.mean(),\n",
    "        'std_score': scores.std()\n",
    "    })\n",
    "    \n",
    "    print(f\"  Time: {elapsed:.2f}s ({elapsed/len(test_queries)*1000:.1f}ms per pair)\")\n",
    "    print(f\"  Scores: mean={scores.mean():.4f}, std={scores.std():.4f}\")\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n‚úì Batch Size Comparison:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd5c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare batch test data\n",
    "n_pairs = 10\n",
    "images_dir = Path('../data/images')\n",
    "test_images = list(images_dir.glob('*.jpg'))[:n_pairs]\n",
    "test_queries = [\n",
    "    \"A photograph of people\",\n",
    "    \"An outdoor scene\",\n",
    "    \"Children playing\",\n",
    "    \"A colorful image\",\n",
    "    \"An action scene\",\n",
    "    \"A landscape view\",\n",
    "    \"People in a setting\",\n",
    "    \"An indoor environment\",\n",
    "    \"A busy scene\",\n",
    "    \"A peaceful moment\"\n",
    "][:n_pairs]\n",
    "\n",
    "print(f\"Testing batch scoring with {n_pairs} pairs...\")\n",
    "print(f\"Found {len(test_images)} test images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c07b4d",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Batch Scoring Tests\n",
    "\n",
    "Test batch processing with different batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173470c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different queries\n",
    "test_queries = [\n",
    "    \"A dog playing in the park\",\n",
    "    \"People at a beach\",\n",
    "    \"A colorful outdoor scene\",\n",
    "    \"Random unrelated text xyz123\",\n",
    "]\n",
    "\n",
    "print(\"Scoring single pairs...\")\n",
    "print(f\"Image: {test_image.name}\\n\")\n",
    "\n",
    "for query in test_queries:\n",
    "    score = encoder.score_pair(query, test_image)\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"Score: {score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcf587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a test image\n",
    "images_dir = Path('../data/images')\n",
    "if not images_dir.exists():\n",
    "    print(f\"‚ö† Images directory not found: {images_dir.resolve()}\")\n",
    "    print(\"Please update the path to your Flickr30K images\")\n",
    "else:\n",
    "    test_images_list = list(images_dir.glob('*.jpg'))[:5]\n",
    "    if len(test_images_list) == 0:\n",
    "        print(\"‚ö† No images found in directory\")\n",
    "    else:\n",
    "        test_image = test_images_list[0]\n",
    "        \n",
    "        # Load and display image\n",
    "        img = Image.open(test_image)\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Test Image: {test_image.name}\")\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Image: {test_image.name}\")\n",
    "        print(f\"Size: {img.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493ef8e9",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Single Pair Scoring\n",
    "\n",
    "Test scoring on a single query-image pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6969bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display model information\n",
    "print(\"Model Information:\")\n",
    "print(f\"  Model: {encoder.model.__class__.__name__}\")\n",
    "print(f\"  Device: {encoder.device}\")\n",
    "print(f\"  FP16: {encoder.use_fp16}\")\n",
    "print(f\"  Batch size: {encoder.batch_size}\")\n",
    "print(f\"  Max batch size: {encoder.max_batch_size}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU Memory:\")\n",
    "    print(f\"  Allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "    print(f\"  Reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21438e1e",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Model Loading & GPU Testing\n",
    "\n",
    "Load the BLIP-2 cross-encoder model and verify it's on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d2a8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path('..').resolve()))\n",
    "\n",
    "print(f\"‚úì Python path configured\")\n",
    "print(f\"Working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42619fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "print(\"‚úì All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfa8ab2",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup & Installation\n",
    "\n",
    "Check that all dependencies are installed and CUDA is available."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
