{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f714ff89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAST Hybrid Search Evaluation Notebook\n",
    "# This notebook replicates scripts/evaluate_accuracy.py in an interactive setting\n",
    "# for quick smoke-test comparison between CLIP-only baseline and Hybrid Search (CLIP + BLIP-2)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure project root is on sys.path\n",
    "project_root = Path(\"..\").resolve()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(\"Project root:\", project_root)\n",
    "print(\"In sys.path:\", any(str(project_root) == p for p in sys.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3926b822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and constants\n",
    "import time\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.retrieval.bi_encoder import BiEncoder\n",
    "from src.retrieval.cross_encoder import CrossEncoder\n",
    "from src.retrieval.faiss_index import FAISSIndex\n",
    "from src.retrieval.hybrid_search import HybridSearchEngine\n",
    "from src.flickr30k.dataset import Flickr30KDataset\n",
    "\n",
    "# FAST MODE constants\n",
    "FAST_SEED = 2025\n",
    "FAST_N = 25\n",
    "FAST_K1 = 30\n",
    "FAST_K2 = 10\n",
    "\n",
    "# Detect data directory (Kaggle vs local)\n",
    "if Path('/kaggle/input').exists():\n",
    "    DATA_DIR = Path('/kaggle/input/flickr30k/data')\n",
    "    print(\"Running on Kaggle; DATA_DIR =\", DATA_DIR)\n",
    "else:\n",
    "    DATA_DIR = project_root / 'data'\n",
    "    print(\"Running locally; DATA_DIR =\", DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5897bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility and evaluation functions (adapted from scripts/evaluate_accuracy.py)\n",
    "\n",
    "def load_components():\n",
    "    \"\"\"Load dataset, encoders, and FAISS index (FAST MODE).\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"LOADING COMPONENTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"\\n[1/4] Loading Flickr30K dataset...\")\n",
    "    dataset = Flickr30KDataset(\n",
    "        images_dir=str(DATA_DIR / 'images'),\n",
    "        captions_file=str(DATA_DIR / 'results.csv')\n",
    "    )\n",
    "    print(f\"  ✓ Loaded {len(dataset)} images\")\n",
    "    \n",
    "    print(\"\\n[2/4] Loading CLIP bi-encoder...\")\n",
    "    bi_encoder = BiEncoder(model_name='ViT-B/32', device='cuda')\n",
    "    print(f\"  ✓ Model: {bi_encoder.model_name}\")\n",
    "    \n",
    "    print(\"\\n[3/4] Loading FAISS index...\")\n",
    "    image_index = FAISSIndex(device='cuda')\n",
    "    index_path = DATA_DIR / 'indices' / 'image_index.faiss'\n",
    "    image_index.load(str(index_path))\n",
    "    print(f\"  ✓ Loaded {image_index.index.ntotal:,} vectors\")\n",
    "    \n",
    "    print(\"\\n[4/4] Loading BLIP-2 cross-encoder...\")\n",
    "    cross_encoder = CrossEncoder(\n",
    "        model_name='Salesforce/blip2-opt-2.7b',\n",
    "        device='cuda',\n",
    "        use_fp16=True\n",
    "    )\n",
    "    print(f\"  ✓ Model: {cross_encoder.model_name}\")\n",
    "    \n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"Components loaded in {load_time:.2f}s\")\n",
    "    print(f\"{'=' * 70}\\n\")\n",
    "    \n",
    "    return bi_encoder, cross_encoder, image_index, dataset\n",
    "\n",
    "\n",
    "def select_test_queries(dataset: Flickr30KDataset, n: int = FAST_N, seed: int = FAST_SEED) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Reproducibly select test queries from the dataset.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"SELECTING {n} TEST QUERIES (FAST MODE)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    unique_images = dataset.get_unique_images()\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = min(n, len(unique_images))\n",
    "    chosen = rng.choice(unique_images, size=n, replace=False)\n",
    "    \n",
    "    test_queries = []\n",
    "    for image_id in chosen:\n",
    "        captions = dataset.get_captions(image_id)\n",
    "        if not captions:\n",
    "            continue\n",
    "        test_queries.append({\n",
    "            'query': captions[0],\n",
    "            'ground_truth': image_id,\n",
    "            'alternatives': captions[1:] if len(captions) > 1 else []\n",
    "        })\n",
    "    \n",
    "    print(f\"\\n✓ Selected {len(test_queries)} test queries\")\n",
    "    return test_queries\n",
    "\n",
    "\n",
    "def ndcg_at_k(rank: int, k: int = 10) -> float:\n",
    "    \"\"\"Calculate nDCG@k for a single relevant item at given rank.\"\"\"\n",
    "    if rank is None or rank > k:\n",
    "        return 0.0\n",
    "    # IDCG is 1.0 for a single relevant item\n",
    "    return 1.0 / np.log2(rank + 1)\n",
    "\n",
    "\n",
    "def calculate_metrics(results: List[Dict[str, Any]], k: int) -> Dict[str, Any]:\n",
    "    \"\"\"Compute Recall@k, MRR, nDCG@10, MAP and latency statistics.\"\"\"\n",
    "    n_queries = len(results)\n",
    "    \n",
    "    recall_at_1 = sum(1 for r in results if r['rank'] == 1) / n_queries\n",
    "    recall_at_5 = sum(1 for r in results if r['rank'] and r['rank'] <= 5) / n_queries\n",
    "    recall_at_10 = sum(1 for r in results if r['rank'] and r['rank'] <= 10) / n_queries\n",
    "    \n",
    "    reciprocal_ranks = [(1.0 / r['rank']) if r['rank'] else 0.0 for r in results]\n",
    "    mrr = float(np.mean(reciprocal_ranks))\n",
    "    \n",
    "    ndcg10 = float(np.mean([ndcg_at_k(r['rank'], k=10) for r in results]))\n",
    "    \n",
    "    # For single relevant item, MAP equals MRR\n",
    "    map_score = mrr\n",
    "    \n",
    "    latencies = [r['latency'] for r in results]\n",
    "    latency_mean = float(np.mean(latencies)) if latencies else 0.0\n",
    "    latency_median = float(np.median(latencies)) if latencies else 0.0\n",
    "    \n",
    "    return {\n",
    "        'recall@1': recall_at_1,\n",
    "        'recall@5': recall_at_5,\n",
    "        'recall@10': recall_at_10,\n",
    "        'mrr': mrr,\n",
    "        'ndcg@10': ndcg10,\n",
    "        'map': map_score,\n",
    "        'latencies': {\n",
    "            'mean': latency_mean,\n",
    "            'median': latency_median,\n",
    "        },\n",
    "        'n_queries': n_queries,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2189b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset, models, and initialize HybridSearchEngine\n",
    "bi_encoder, cross_encoder, image_index, dataset = load_components()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Initializing HybridSearchEngine (FAST MODE)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "engine = HybridSearchEngine(\n",
    "    bi_encoder=bi_encoder,\n",
    "    cross_encoder=cross_encoder,\n",
    "    image_index=image_index,\n",
    "    dataset=dataset,\n",
    "    config={\n",
    "        'k1': FAST_K1,\n",
    "        'k2': FAST_K2,\n",
    "        'batch_size': 8,\n",
    "        'use_cache': False,\n",
    "        'show_progress': False,\n",
    "        'fusion_method': 'weighted',\n",
    "        'stage1_weight': 0.3,\n",
    "        'stage2_weight': 0.7,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"  ✓ Engine initialized\")\n",
    "print(f\"  ✓ Images in index: {image_index.index.ntotal:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c175b326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select test queries\n",
    "test_queries = select_test_queries(dataset, n=FAST_N, seed=FAST_SEED)\n",
    "\n",
    "# Preview first 5 queries\n",
    "preview_rows = []\n",
    "for tq in test_queries[:5]:\n",
    "    preview_rows.append({\n",
    "        'query': tq['query'],\n",
    "        'ground_truth': tq['ground_truth'],\n",
    "        'n_alternatives': len(tq['alternatives']),\n",
    "    })\n",
    "\n",
    "pd.DataFrame(preview_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003dc64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP-only evaluation (baseline)\n",
    "results_clip: List[Dict[str, Any]] = []\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EVALUATING CLIP-ONLY SEARCH (Baseline)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Evaluating {len(test_queries)} queries with k={FAST_K2}\")\n",
    "\n",
    "for tq in test_queries:\n",
    "    query = tq['query']\n",
    "    ground_truth = tq['ground_truth']\n",
    "    \n",
    "    start = time.time()\n",
    "    # Use engine's stage-1 CLIP + FAISS retrieval\n",
    "    search_results = engine._stage1_retrieve(query, k1=FAST_K2)\n",
    "    latency_ms = (time.time() - start) * 1000\n",
    "    \n",
    "    retrieved_ids = [img_id for img_id, score in search_results]\n",
    "    \n",
    "    ground_truth_rank = None\n",
    "    if ground_truth in retrieved_ids:\n",
    "        ground_truth_rank = retrieved_ids.index(ground_truth) + 1\n",
    "    \n",
    "    results_clip.append({\n",
    "        'query': query,\n",
    "        'ground_truth': ground_truth,\n",
    "        'retrieved': retrieved_ids,\n",
    "        'rank': ground_truth_rank,\n",
    "        'latency': latency_ms,\n",
    "    })\n",
    "\n",
    "metrics_clip = calculate_metrics(results_clip, k=FAST_K2)\n",
    "\n",
    "print(\"\\nCLIP-only metrics (FAST MODE):\")\n",
    "for k, v in metrics_clip.items():\n",
    "    if k == 'latencies':\n",
    "        print(f\"  {k}: mean={v['mean']:.2f} ms, median={v['median']:.2f} ms\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f2c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid evaluation (CLIP + BLIP-2)\n",
    "results_hybrid: List[Dict[str, Any]] = []\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EVALUATING HYBRID SEARCH (CLIP + BLIP-2)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Evaluating {len(test_queries)} queries with k1={FAST_K1}, k2={FAST_K2}\")\n",
    "\n",
    "for tq in test_queries:\n",
    "    query = tq['query']\n",
    "    ground_truth = tq['ground_truth']\n",
    "    \n",
    "    start = time.time()\n",
    "    search_results = engine.text_to_image_hybrid_search(\n",
    "        query=query,\n",
    "        k1=FAST_K1,\n",
    "        k2=FAST_K2,\n",
    "        show_progress=False,\n",
    "    )\n",
    "    latency_ms = (time.time() - start) * 1000\n",
    "    \n",
    "    retrieved_ids = [img_id for img_id, score in search_results]\n",
    "    \n",
    "    ground_truth_rank = None\n",
    "    if ground_truth in retrieved_ids:\n",
    "        ground_truth_rank = retrieved_ids.index(ground_truth) + 1\n",
    "    \n",
    "    results_hybrid.append({\n",
    "        'query': query,\n",
    "        'ground_truth': ground_truth,\n",
    "        'retrieved': retrieved_ids,\n",
    "        'rank': ground_truth_rank,\n",
    "        'latency': latency_ms,\n",
    "    })\n",
    "\n",
    "metrics_hybrid = calculate_metrics(results_hybrid, k=FAST_K2)\n",
    "\n",
    "print(\"\\nHybrid search metrics (FAST MODE):\")\n",
    "for k, v in metrics_hybrid.items():\n",
    "    if k == 'latencies':\n",
    "        print(f\"  {k}: mean={v['mean']:.2f} ms, median={v['median']:.2f} ms\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01647bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate and compare metrics\n",
    "rows = []\n",
    "for name, m in [('clip_only', metrics_clip), ('hybrid', metrics_hybrid)]:\n",
    "    rows.append({\n",
    "        'method': name,\n",
    "        'recall@1': m['recall@1'],\n",
    "        'recall@5': m['recall@5'],\n",
    "        'recall@10': m['recall@10'],\n",
    "        'mrr': m['mrr'],\n",
    "        'ndcg@10': m['ndcg@10'],\n",
    "        'map': m['map'],\n",
    "        'latency_mean_ms': m['latencies']['mean'],\n",
    "        'latency_median_ms': m['latencies']['median'],\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(rows).set_index('method')\n",
    "display(metrics_df)\n",
    "\n",
    "# Calculate deltas (hybrid - clip_only)\n",
    "delta = metrics_df.loc['hybrid'] - metrics_df.loc['clip_only']\n",
    "print(\"\\nDeltas (hybrid - clip_only):\")\n",
    "display(delta.to_frame(name='delta'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a0d6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Bar chart for Recall@10\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(\n",
    "    x=metrics_df.index,\n",
    "    y=metrics_df['recall@10'],\n",
    "    palette=['C0', 'C1']\n",
    ")\n",
    "plt.title('Recall@10 Comparison (FAST MODE)')\n",
    "plt.ylabel('Recall@10')\n",
    "plt.xlabel('Method')\n",
    "for i, val in enumerate(metrics_df['recall@10']):\n",
    "    plt.text(i, val + 0.01, f\"{val:.2f}\", ha='center')\n",
    "plt.ylim(0, min(1.0, metrics_df['recall@10'].max() + 0.1))\n",
    "plt.show()\n",
    "\n",
    "# Bar chart for Mean Latency\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.barplot(\n",
    "    x=metrics_df.index,\n",
    "    y=metrics_df['latency_mean_ms'],\n",
    "    palette=['C0', 'C1']\n",
    ")\n",
    "plt.title('Mean Latency Comparison (FAST MODE)')\n",
    "plt.ylabel('Mean latency (ms)')\n",
    "plt.xlabel('Method')\n",
    "for i, val in enumerate(metrics_df['latency_mean_ms']):\n",
    "    plt.text(i, val + 1, f\"{val:.1f}\", ha='center')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
