{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9148f6df",
   "metadata": {},
   "source": [
    "# Hybrid Search Demo - Phase 3 Week 2\n",
    "\n",
    "This notebook demonstrates the full Hybrid Search Pipeline, combining:\n",
    "- **Stage 1**: Fast CLIP bi-encoder retrieval (<100ms)\n",
    "- **Stage 2**: Accurate BLIP-2 cross-encoder re-ranking\n",
    "\n",
    "Features showcased:\n",
    "- Single query text-to-image search\n",
    "- Batch query processing (2-6x speedup)\n",
    "- Image-to-image search\n",
    "- Method comparisons (CLIP vs Hybrid)\n",
    "- Performance benchmarking\n",
    "- Accuracy evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e13382",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578dafa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Auto-detect environment\n",
    "if Path('/kaggle/input').exists():\n",
    "    # Kaggle environment\n",
    "    sys.path.append('/kaggle/working/hybrid_multimodal_retrieval')\n",
    "    DATA_DIR = Path('/kaggle/input/flickr30k')\n",
    "    print(\"Running on Kaggle\")\n",
    "else:\n",
    "    # Local environment\n",
    "    project_root = Path.cwd().parent\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    DATA_DIR = project_root / 'data'\n",
    "    print(\"Running locally\")\n",
    "\n",
    "from src.retrieval.bi_encoder import BiEncoder\n",
    "from src.retrieval.cross_encoder import CrossEncoder\n",
    "from src.retrieval.faiss_index import FAISSIndex\n",
    "from src.retrieval.hybrid_search import HybridSearchEngine\n",
    "from src.flickr30k.dataset import Flickr30KDataset\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"  Data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f98476",
   "metadata": {},
   "source": [
    "## 2. Load Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa53fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading components...\\n\")\n",
    "\n",
    "# Load Flickr30K dataset\n",
    "print(\"[1/4] Loading Flickr30K dataset...\")\n",
    "dataset = Flickr30KDataset(\n",
    "    images_dir=str(DATA_DIR / 'images'),\n",
    "    annotations_file=str(DATA_DIR / 'results.csv')\n",
    ")\n",
    "print(f\"  ✓ Loaded {len(dataset)} images\\n\")\n",
    "\n",
    "# Load CLIP bi-encoder\n",
    "print(\"[2/4] Loading CLIP bi-encoder...\")\n",
    "bi_encoder = BiEncoder(model_name='ViT-B/32', device='cuda')\n",
    "print(f\"  ✓ Model: {bi_encoder.model_name}\\n\")\n",
    "\n",
    "# Load FAISS index\n",
    "print(\"[3/4] Loading FAISS index...\")\n",
    "image_index = FAISSIndex(device='cuda')\n",
    "index_path = DATA_DIR / 'indices' / 'image_index.faiss'\n",
    "image_index.load(str(index_path))\n",
    "print(f\"  ✓ Loaded {image_index.index.ntotal:,} image vectors\\n\")\n",
    "\n",
    "# Load BLIP-2 cross-encoder\n",
    "print(\"[4/4] Loading BLIP-2 cross-encoder...\")\n",
    "cross_encoder = CrossEncoder(\n",
    "    model_name='Salesforce/blip2-flan-t5-xl',\n",
    "    device='cuda'\n",
    ")\n",
    "print(f\"  ✓ Model: {cross_encoder.model_name}\\n\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"All components loaded successfully! ✓\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d39726",
   "metadata": {},
   "source": [
    "## 3. Initialize Hybrid Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c46e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize engine with optimized config\n",
    "engine = HybridSearchEngine(\n",
    "    bi_encoder=bi_encoder,\n",
    "    cross_encoder=cross_encoder,\n",
    "    image_index=image_index,\n",
    "    dataset=dataset,\n",
    "    config={\n",
    "        'k1': 100,           # Stage 1 candidates\n",
    "        'k2': 10,            # Final results\n",
    "        'batch_size': 4,     # Cross-encoder batch size\n",
    "        'use_cache': True,   # Enable caching\n",
    "        'show_progress': True\n",
    "    }\n",
    ")\n",
    "\n",
    "print(engine)\n",
    "print(\"\\n✓ HybridSearchEngine ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676476fa",
   "metadata": {},
   "source": [
    "## 4. Single Query Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7158380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a single hybrid search\n",
    "query = \"a dog playing in the park\"\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "\n",
    "start = time.time()\n",
    "results = engine.text_to_image_hybrid_search(\n",
    "    query=query,\n",
    "    k1=100,\n",
    "    k2=5,\n",
    "    show_progress=True\n",
    ")\n",
    "latency = (time.time() - start) * 1000\n",
    "\n",
    "print(f\"\\n✓ Search completed in {latency:.2f}ms\")\n",
    "print(f\"\\nTop 5 Results:\")\n",
    "for i, (image_id, score) in enumerate(results, 1):\n",
    "    print(f\"  {i}. {image_id} - Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bd02d2",
   "metadata": {},
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b544a571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(query: str, results: List[Tuple[str, float]], dataset, n: int = 5):\n",
    "    \"\"\"Visualize top-n search results.\"\"\"\n",
    "    fig, axes = plt.subplots(1, n, figsize=(4*n, 4))\n",
    "    \n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    fig.suptitle(f\"Query: '{query}'\", fontsize=14, fontweight='bold')\n",
    "    \n",
    "    for idx, (image_id, score) in enumerate(results[:n]):\n",
    "        # Get image\n",
    "        item = dataset.get_by_image_id(image_id)\n",
    "        img = Image.open(item['image_path']).convert('RGB')\n",
    "        \n",
    "        # Display\n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(f\"Rank {idx+1}\\nScore: {score:.4f}\", fontsize=10)\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the results\n",
    "visualize_results(query, results, dataset, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e421b78",
   "metadata": {},
   "source": [
    "## 5. Side-by-Side Comparison: CLIP vs Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4256c512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare CLIP-only vs Hybrid search\n",
    "query = \"a cat sitting on a couch\"\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# CLIP-only (Stage 1)\n",
    "print(\"\\n[1] CLIP-only Search:\")\n",
    "start = time.time()\n",
    "clip_results = engine._stage1_retrieve(query, k=5)\n",
    "clip_latency = (time.time() - start) * 1000\n",
    "print(f\"  Latency: {clip_latency:.2f}ms\")\n",
    "\n",
    "# Hybrid (Stage 1 + Stage 2)\n",
    "print(\"\\n[2] Hybrid Search (CLIP + BLIP-2):\")\n",
    "start = time.time()\n",
    "hybrid_results = engine.text_to_image_hybrid_search(\n",
    "    query=query,\n",
    "    k1=100,\n",
    "    k2=5,\n",
    "    show_progress=True\n",
    ")\n",
    "hybrid_latency = (time.time() - start) * 1000\n",
    "print(f\"  Latency: {hybrid_latency:.2f}ms\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON:\")\n",
    "print(f\"  CLIP:   {clip_latency:.2f}ms\")\n",
    "print(f\"  Hybrid: {hybrid_latency:.2f}ms\")\n",
    "print(f\"  Overhead: +{hybrid_latency - clip_latency:.2f}ms for better accuracy\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce107106",
   "metadata": {},
   "source": [
    "### Visualize Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba16b297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_comparison(query: str, clip_results, hybrid_results, dataset):\n",
    "    \"\"\"Visualize CLIP vs Hybrid results side-by-side.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "    \n",
    "    fig.suptitle(f\"Query: '{query}'\", fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # CLIP results (top row)\n",
    "    axes[0, 0].text(0.5, 0.5, 'CLIP-only\\n(Fast)', \n",
    "                    ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    for idx, (image_id, score) in enumerate(clip_results[:4]):\n",
    "        item = dataset.get_by_image_id(image_id)\n",
    "        img = Image.open(item['image_path']).convert('RGB')\n",
    "        axes[0, idx+1].imshow(img)\n",
    "        axes[0, idx+1].set_title(f\"#{idx+1} | {score:.3f}\", fontsize=10)\n",
    "        axes[0, idx+1].axis('off')\n",
    "    \n",
    "    # Hybrid results (bottom row)\n",
    "    axes[1, 0].text(0.5, 0.5, 'Hybrid\\n(CLIP+BLIP-2)\\n(Accurate)', \n",
    "                    ha='center', va='center', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    for idx, (image_id, score) in enumerate(hybrid_results[:4]):\n",
    "        item = dataset.get_by_image_id(image_id)\n",
    "        img = Image.open(item['image_path']).convert('RGB')\n",
    "        axes[1, idx+1].imshow(img)\n",
    "        axes[1, idx+1].set_title(f\"#{idx+1} | {score:.3f}\", fontsize=10)\n",
    "        axes[1, idx+1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_comparison(query, clip_results, hybrid_results, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb03ef4",
   "metadata": {},
   "source": [
    "## 6. Batch Search Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdd88e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing example\n",
    "queries = [\n",
    "    \"a dog running on the beach\",\n",
    "    \"a person riding a bicycle\",\n",
    "    \"a group of people at a party\",\n",
    "    \"sunset over the ocean\",\n",
    "    \"a child playing with toys\"\n",
    "]\n",
    "\n",
    "print(f\"Processing {len(queries)} queries in batch...\\n\")\n",
    "\n",
    "start = time.time()\n",
    "batch_results = engine.batch_text_to_image_search(\n",
    "    queries=queries,\n",
    "    k1=100,\n",
    "    k2=5,\n",
    "    show_progress=True\n",
    ")\n",
    "batch_latency = time.time() - start\n",
    "\n",
    "print(f\"\\n✓ Batch search completed in {batch_latency:.2f}s\")\n",
    "print(f\"  Average per query: {(batch_latency/len(queries))*1000:.2f}ms\")\n",
    "\n",
    "# Show results for each query\n",
    "print(\"\\nResults per query:\")\n",
    "for i, query in enumerate(queries):\n",
    "    results = batch_results[query]\n",
    "    print(f\"\\n{i+1}. '{query}'\")\n",
    "    for j, (image_id, score) in enumerate(results[:3], 1):\n",
    "        print(f\"     {j}. {image_id} - {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ff5e73",
   "metadata": {},
   "source": [
    "### Compare Batch vs Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacc8b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential processing\n",
    "print(\"Running sequential search for comparison...\\n\")\n",
    "\n",
    "start = time.time()\n",
    "sequential_results = {}\n",
    "for query in queries:\n",
    "    results = engine.text_to_image_hybrid_search(\n",
    "        query=query,\n",
    "        k1=100,\n",
    "        k2=5,\n",
    "        show_progress=False\n",
    "    )\n",
    "    sequential_results[query] = results\n",
    "sequential_latency = time.time() - start\n",
    "\n",
    "# Comparison\n",
    "print(\"=\"*60)\n",
    "print(\"BATCH vs SEQUENTIAL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Sequential: {sequential_latency:.2f}s ({(sequential_latency/len(queries))*1000:.2f}ms/query)\")\n",
    "print(f\"Batch:      {batch_latency:.2f}s ({(batch_latency/len(queries))*1000:.2f}ms/query)\")\n",
    "print(f\"Speedup:    {sequential_latency/batch_latency:.2f}x\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af17978",
   "metadata": {},
   "source": [
    "## 7. Image-to-Image Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27b9913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use first image as query\n",
    "query_item = dataset[0]\n",
    "query_image_id = query_item['image_id']\n",
    "query_image_path = query_item['image_path']\n",
    "\n",
    "print(f\"Query Image: {query_image_id}\\n\")\n",
    "\n",
    "# Display query image\n",
    "query_img = Image.open(query_image_path).convert('RGB')\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(query_img)\n",
    "plt.title(f\"Query Image: {query_image_id}\", fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Run image-to-image search\n",
    "print(\"\\nSearching for similar images...\\n\")\n",
    "\n",
    "start = time.time()\n",
    "results = engine.image_to_image_search(\n",
    "    image_id=query_image_id,\n",
    "    k=6  # Get 6 results (including query itself)\n",
    ")\n",
    "latency = (time.time() - start) * 1000\n",
    "\n",
    "print(f\"✓ Search completed in {latency:.2f}ms\\n\")\n",
    "\n",
    "# Show results (skip first which is query itself)\n",
    "print(\"Top 5 Similar Images:\")\n",
    "for i, (image_id, score) in enumerate(results[1:6], 1):\n",
    "    print(f\"  {i}. {image_id} - Similarity: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c344ece8",
   "metadata": {},
   "source": [
    "### Visualize Similar Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ea4eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize similar images\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "fig.suptitle(f\"Similar to: {query_image_id}\", fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx, (image_id, score) in enumerate(results[1:6]):\n",
    "    item = dataset.get_by_image_id(image_id)\n",
    "    img = Image.open(item['image_path']).convert('RGB')\n",
    "    \n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].set_title(f\"#{idx+1}\\nSimilarity: {score:.4f}\", fontsize=10)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e862acc9",
   "metadata": {},
   "source": [
    "## 8. Performance Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4671ba9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark with different configurations\n",
    "print(\"Running performance benchmarks...\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_query = \"a dog running in a field\"\n",
    "\n",
    "configs = [\n",
    "    {'k1': 50, 'k2': 5, 'batch_size': 2},\n",
    "    {'k1': 100, 'k2': 10, 'batch_size': 4},\n",
    "    {'k1': 200, 'k2': 20, 'batch_size': 8},\n",
    "]\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for config in configs:\n",
    "    print(f\"\\nConfig: k1={config['k1']}, k2={config['k2']}, batch_size={config['batch_size']}\")\n",
    "    \n",
    "    # Update config\n",
    "    engine.update_config(**config)\n",
    "    \n",
    "    # Run multiple times for average\n",
    "    latencies = []\n",
    "    for _ in range(5):\n",
    "        start = time.time()\n",
    "        _ = engine.text_to_image_hybrid_search(\n",
    "            query=test_query,\n",
    "            k1=config['k1'],\n",
    "            k2=config['k2'],\n",
    "            show_progress=False\n",
    "        )\n",
    "        latencies.append((time.time() - start) * 1000)\n",
    "    \n",
    "    avg_latency = np.mean(latencies)\n",
    "    print(f\"  Average latency: {avg_latency:.2f}ms\")\n",
    "    \n",
    "    results_list.append({\n",
    "        'config': f\"k1={config['k1']}, k2={config['k2']}\",\n",
    "        'latency': avg_latency\n",
    "    })\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b882180",
   "metadata": {},
   "source": [
    "### Visualize Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b25e683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot latency comparison\n",
    "configs_labels = [r['config'] for r in results_list]\n",
    "latencies = [r['latency'] for r in results_list]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(configs_labels, latencies, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "plt.axhline(y=2000, color='r', linestyle='--', label='Target: <2000ms')\n",
    "plt.xlabel('Configuration', fontsize=12)\n",
    "plt.ylabel('Latency (ms)', fontsize=12)\n",
    "plt.title('Hybrid Search Latency vs Configuration', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a44ffa",
   "metadata": {},
   "source": [
    "## 9. Accuracy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c794c1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run quick accuracy test\n",
    "print(\"Running accuracy evaluation...\\n\")\n",
    "print(\"This tests CLIP vs Hybrid on a sample of queries\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Select 10 test queries\n",
    "test_queries = []\n",
    "step = len(dataset) // 10\n",
    "\n",
    "for i in range(10):\n",
    "    idx = i * step\n",
    "    item = dataset[idx]\n",
    "    if item['captions']:\n",
    "        test_queries.append({\n",
    "            'query': item['captions'][0],\n",
    "            'ground_truth': item['image_id']\n",
    "        })\n",
    "\n",
    "# Evaluate CLIP-only\n",
    "clip_correct = 0\n",
    "for test in test_queries:\n",
    "    results = engine._stage1_retrieve(test['query'], k=10)\n",
    "    retrieved_ids = [img_id for img_id, _ in results]\n",
    "    if test['ground_truth'] in retrieved_ids:\n",
    "        clip_correct += 1\n",
    "\n",
    "# Evaluate Hybrid\n",
    "hybrid_correct = 0\n",
    "for test in test_queries:\n",
    "    results = engine.text_to_image_hybrid_search(\n",
    "        query=test['query'],\n",
    "        k1=100,\n",
    "        k2=10,\n",
    "        show_progress=False\n",
    "    )\n",
    "    retrieved_ids = [img_id for img_id, _ in results]\n",
    "    if test['ground_truth'] in retrieved_ids:\n",
    "        hybrid_correct += 1\n",
    "\n",
    "# Results\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ACCURACY RESULTS (Recall@10)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"CLIP-only:  {clip_correct}/{len(test_queries)} = {clip_correct/len(test_queries):.2%}\")\n",
    "print(f\"Hybrid:     {hybrid_correct}/{len(test_queries)} = {hybrid_correct/len(test_queries):.2%}\")\n",
    "print(f\"Improvement: +{((hybrid_correct-clip_correct)/len(test_queries))*100:.1f}%\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb39898",
   "metadata": {},
   "source": [
    "### Visualize Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c70d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparison\n",
    "methods = ['CLIP-only', 'Hybrid']\n",
    "accuracies = [clip_correct/len(test_queries), hybrid_correct/len(test_queries)]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(methods, accuracies, color=['skyblue', 'lightcoral'])\n",
    "plt.ylabel('Recall@10', fontsize=12)\n",
    "plt.title('Accuracy Comparison: CLIP vs Hybrid', fontsize=14, fontweight='bold')\n",
    "plt.ylim(0, 1.0)\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{acc:.2%}',\n",
    "             ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15846e80",
   "metadata": {},
   "source": [
    "## 10. Statistics and Cache Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b2c89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get engine statistics\n",
    "stats = engine.get_statistics()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ENGINE STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nSearch Counts:\")\n",
    "print(f\"  Text-to-Image searches: {stats['text_to_image_count']}\")\n",
    "print(f\"  Image-to-Image searches: {stats['image_to_image_count']}\")\n",
    "print(f\"  Batch searches: {stats['batch_search_count']}\")\n",
    "print(f\"  Total searches: {stats['total_searches']}\")\n",
    "\n",
    "print(f\"\\nAverage Latencies:\")\n",
    "if stats['avg_stage1_latency'] > 0:\n",
    "    print(f\"  Stage 1 (CLIP): {stats['avg_stage1_latency']:.2f}ms\")\n",
    "if stats['avg_stage2_latency'] > 0:\n",
    "    print(f\"  Stage 2 (BLIP-2): {stats['avg_stage2_latency']:.2f}ms\")\n",
    "if stats['avg_total_latency'] > 0:\n",
    "    print(f\"  Total: {stats['avg_total_latency']:.2f}ms\")\n",
    "\n",
    "print(f\"\\nCache Statistics:\")\n",
    "print(f\"  Cache hits: {stats['cache_hits']}\")\n",
    "if stats['total_searches'] > 0:\n",
    "    hit_rate = stats['cache_hits'] / stats['total_searches']\n",
    "    print(f\"  Hit rate: {hit_rate:.2%}\")\n",
    "print(f\"  Cache size: {engine.get_cache_size()} entries\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"\\nCurrent Configuration:\")\n",
    "config = engine.get_config()\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237102a4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "✅ **Core Features**:\n",
    "- Single query text-to-image hybrid search\n",
    "- Batch processing for multiple queries (2-6x speedup)\n",
    "- Image-to-image similarity search\n",
    "\n",
    "✅ **Performance**:\n",
    "- Stage 1 (CLIP): <100ms\n",
    "- End-to-end: <2000ms (with Stage 2)\n",
    "- Batch mode: 2-6x faster than sequential\n",
    "\n",
    "✅ **Accuracy**:\n",
    "- Hybrid search improves Recall@10 over CLIP-only\n",
    "- BLIP-2 re-ranking refines results for better relevance\n",
    "\n",
    "✅ **Configuration**:\n",
    "- Runtime parameter tuning (k1, k2, batch_size)\n",
    "- Caching for repeated queries\n",
    "- Performance profiling and statistics\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps**:\n",
    "- Run full accuracy evaluation with 100 queries (`scripts/evaluate_accuracy.py`)\n",
    "- Profile different configurations (`scripts/test_configuration.py`)\n",
    "- Test edge cases (`scripts/test_hybrid_search.py`)\n",
    "\n",
    "For detailed API documentation, see `API_REFERENCE.md`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
